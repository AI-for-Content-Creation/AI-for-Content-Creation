openreview_number,title,authorids,authors,keywords,abstract,accepted_elsewhere,publication_conference_if_published,decision,award,arXiv,webpage,poster_session
2,LocInv: Localization-aware Inversion for Text-Guided Image Editing,~Chuanming_Tang1|~Kai_Wang7|~Fei_Yang4|~Joost_van_de_Weijer5,Chuanming Tang|Kai Wang|Fei Yang|Joost van de Weijer,diffusion models|image editing|inversion,"Large-scale Text-to-Image (T2I) diffusion models demonstrate significant generation capabilities based on textual prompts. Based on the T2I diffusion models, text-guided image editing research aims to empower users to manipulate generated images by altering the text prompts. However, existing image editing techniques are prone to editing over unintentional regions that are beyond the intended target area, primarily due to inaccuracies in cross-attention maps. To address this problem, we propose Localization-aware Inversion (LocInv), which exploits segmentation maps or bounding boxes as extra localization priors to refine the cross-attention maps in the denoising phases of the diffusion process. Through the dynamic updating of tokens corresponding to noun words in the textual input, we are compelling the cross-attention maps to closely align with the correct noun and adjective words in the text prompt. Based on this technique, we achieve fine-grained image editing over particular objects while preventing undesired changes to other regions. Our method LocInv, based on the publicly available Stable Diffusion, is extensively evaluated on a subset of the COCO dataset, and consistently obtains superior results both quantitatively and qualitatively. We show improved prompt editing results for Word-Swap and Attribute-Edit, especially in complex multi-object scenes.",No,,Accept (Poster),,https://arxiv.org/abs/2405.01496,https://github.com/wangkai930418/DPL,1
5,Visual Style Prompting with Swapping Self-Attention,~Jaeseok_Jeong2|~Junho_Kim3|~Youngjung_Uh2|~Yunjey_Choi3|~Gayoung_Lee1,Jaeseok Jeong|Junho Kim|Youngjung Uh|Yunjey Choi|Gayoung Lee,generative models|style transfer,"In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation.
Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, visual style prompting, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately.",No,,Accept (Poster),,,,2
6,Automated Virtual Product Placement and Assessment in Images using Diffusion Models,~Mohammad_Mahmudul_Alam1|~Negin_Sokhandan2|~Emmett_D._Goodman1,Mohammad Mahmudul Alam|Negin Sokhandan|Emmett D. Goodman,Virtual Product Placement|Stable Diffusion|Generative AI|Advertisement|Content Creation,"In Virtual Product Placement (VPP) applications, the discrete integration of specific brand products into images or videos has emerged as a challenging yet important task. This paper introduces a novel three-stage fully automated VPP system. In the first stage, a language-guided image segmentation model identifies optimal regions within images for product inpainting. In the second stage, Stable Diffusion (SD), fine-tuned with a few example product images, is used to inpaint the product into the previously identified candidate regions. The final stage introduces an 'Alignment Module', which is designed to effectively sieve out low-quality images. Comprehensive experiments demonstrate that the Alignment Module ensures the presence of the intended product in every generated image and enhances the average quality of images by 35%. The results presented in this paper demonstrate the effectiveness of the proposed VPP system, which holds significant potential for transforming the landscape of virtual advertising and marketing strategies.",Yes,CVIV24,Accept (Poster),,,,1
7,VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,~Yuchao_Gu1|~Yipin_Zhou4|~Bichen_Wu1|~Licheng_Yu4|~Jia-Wei_Liu1|~Rui_Zhao12|~Jay_Zhangjie_Wu1|~David_Junhao_Zhang1|~Mike_Zheng_Shou1|~Kevin_Dechau_Tang1,Yuchao Gu|Yipin Zhou|Bichen Wu|Licheng Yu|Jia-Wei Liu|Rui Zhao|Jay Zhangjie Wu|David Junhao Zhang|Mike Zheng Shou|Kevin Dechau Tang,Image and video synthesis and generation,"Current diffusion-based video editing primarily focuses on structure-preserved editing by utilizing various dense correspondences to ensure temporal consistency and motion alignment. However, these approaches are often ineffective when the target edit involves a shape change. To embark on video editing with shape change, we explore customized video subject swapping in this work, where we aim to replace the main subject in a source video with a target subject having a distinct identity and potentially different shape. In contrast to previous methods that rely on dense correspondences, we introduce the VideoSwap framework that exploits semantic point correspondences, inspired by our observation that only a small number of semantic points are necessary to align the subject's motion trajectory and modify its shape. We also introduce various user-point interactions (\eg, removing points and dragging points) to address various semantic point correspondence. Extensive experiments demonstrate state-of-the-art video subject swapping results across a variety of real-world videos.",Yes,CVPR24,Accept (Poster),,,,2
9,As-Plausible-As-Possible: Plausibility-Aware Mesh Deformation Using 2D Diffusion Priors,~Seungwoo_Yoo1|~Kunho_Kim3|~Vladimir_Kim1|~Minhyuk_Sung1,Seungwoo Yoo|Kunho Kim|Vladimir Kim|Minhyuk Sung,mesh deformation|jacobian fields|score-distillation|generative modeling,"We present As-Plausible-as-Possible (APAP) mesh deformation technique that leverages 2D diffusion priors to preserve the plausibility of a mesh under user-controlled deformation. Our framework uses per-face Jacobians to represent mesh deformations, where mesh vertex coordinates are computed via a differentiable Poisson Solve. The deformed mesh is rendered, and the resulting 2D image is used in the Score Distillation Sampling (SDS) process, which enables extracting meaningful plausibility priors from a pretrained 2D diffusion model. To better preserve the identity of the edited mesh, we fine-tune our 2D diffusion model with LoRA. Gradients extracted by SDS and a userprescribed handle displacement are then backpropagated to the per-face Jacobians, and we use iterative gradient descent to compute the final deformation that balances between the user edit and the output plausibility. We evaluate our method with 2D and 3D meshes and demonstrate qualitative and quantitative improvements when using plausibility priors over geometry-preservation or distortionminimization priors used by previous techniques.",Yes,CVPR24,Accept (Poster),,https://arxiv.org/abs/2311.16739,https://as-plausible-as-possible.github.io/,1
11,ReasonPix2Pix: Instruction Reasoning Dataset for Advanced Image Editing,~Ying_Jin1|~Pengyang_Ling1|~Xiaoyi_Dong1|~Pan_Zhang1|~Dahua_Lin1|~Jiaqi_Wang1,Ying Jin|Pengyang Ling|Xiaoyi Dong|Pan Zhang|Dahua Lin|Jiaqi Wang,instruction editing|reasoning editing,"Instruction-based image editing focuses on equipping a generative model with the capacity to adhere to human-written instructions for editing images. Current approaches typically comprehend explicit and specific instructions. However, they often exhibit a deficiency in executing active reasoning capacities required to comprehend instructions that are implicit or insufficiently defined. To enhance active reasoning capabilities and impart intelligence to the editing model, we introduce ReasonPix2Pix, a comprehensive reasoning-attentive instruction editing dataset. The dataset is characterised by 1) reasoning instruction, 2) more realistic images from fine-grained categories, and 3) increased variances between input and edited images. When fine-tuned with the our dataset under supervised conditions, the model demonstrates superior performance in instructional editing tasks, independent of whether the tasks require reasoning or not. The code, model, and dataset will be publicly available.",No,,Accept (Poster),,https://arxiv.org/abs/2405.11190,,2
12,MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model,~Zhongcong_Xu1|~Jianfeng_Zhang3|~Jun_Hao_Liew1|~Hanshu_Yan1|~Jia-Wei_Liu1|~Chenxu_Zhang1|~Jiashi_Feng1|~Mike_Zheng_Shou1,Zhongcong Xu|Jianfeng Zhang|Jun Hao Liew|Hanshu Yan|Jia-Wei Liu|Chenxu Zhang|Jiashi Feng|Mike Zheng Shou,image animation,"This paper studies the human image animation task, which aims to generate a video of a certain reference identity following a particular motion sequence. Existing animation works typically employ the frame-warping technique to animate the reference image towards the target motion. Despite achieving reasonable results, these approaches face challenges in maintaining temporal consistency throughout the animation due to the lack of temporal modeling and poor preservation of reference identity. In this work, we introduce \ours{}, a diffusion-based framework that aims at enhancing temporal consistency, preserving reference image faithfully, and improving animation fidelity. To achieve this, we first develop a video diffusion model to encode temporal information. Second, to maintain the appearance coherence across frames, we introduce a novel appearance encoder to retain the intricate details of the reference image. Leveraging these two innovations, we further employ a simple video fusion technique to encourage smooth transitions for long video animation. Empirical results demonstrate the superiority of our method over baseline approaches on two benchmarks. Notably, our approach outperforms the strongest baseline by over 38\% in terms of video fidelity on the challenging TikTok dancing dataset. Code and model will be made available.",Yes,CVPR24,Accept (Poster),,,,1
13,ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer,~Bumsoo_Kim2|~Abdul_Muqeet1|~Kyuchul_Lee1|~Sanghyun_Seo1,Bumsoo Kim|Abdul Muqeet|Kyuchul Lee|Sanghyun Seo,Portrait Style Transfer|Face Re-Aging,"Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the lack of a network that can seamlessly edit the apparent age in NPR images has limited these tasks to a naive, sequential approach. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance. By adopting an exemplar-based approach, our method offers greater flexibility compared to domain-level fine-tuning approaches, which typically require separate training or fine-tuning for each domain. This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization. Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the style of examples, maintaining both natural appearance and controllability.",No,,Accept (Poster),,https://arxiv.org/abs/2402.02733,https://gh-bumsookim.github.io/ToonAging/,2
14,Towards a Perceptual Evaluation Framework for Lighting Estimation,~Justine_Giroux1|~Mohammad_Reza_Karimi_Dastjerdi1|~Yannick_Hold-Geoffroy2|~Javier_Vazquez-Corral1|~Jean-Francois_Lalonde2,Justine Giroux|Mohammad Reza Karimi Dastjerdi|Yannick Hold-Geoffroy|Javier Vazquez-Corral|Jean-Francois Lalonde,Vision|Graphics|Lighting Estimation|IQA Metrics|Perception,"Progress in lighting estimation is tracked by computing existing image quality assessment (IQA) metrics on images from standard datasets. While this may appear to be a reasonable approach, we demonstrate that doing so does not correlate to human preference when the estimated lighting is used to relight a virtual scene into a real photograph. To study this, we design a controlled psychophysical experiment where human observers must choose their preference amongst rendered scenes lit using a set of lighting estimation algorithms selected from the recent literature, and use it to analyse how these algorithms perform according to human perception. Then, we demonstrate that none of the most popular IQA metrics from the literature, taken individually, correctly represent human perception. Finally, we show that by learning a combination of existing IQA metrics, we can more accurately represent human preference. This provides a new perceptual framework to help evaluate future lighting estimation algorithms.",Yes,CVPR24,Accept (Poster),,https://arxiv.org/abs/2312.04334,https://github.com/JustineGiroux/Lightsome,2
15,Customize Your Own Paired Data via Few-shot Way,~jinshuchen1|~Bingchuan_Li2|~Miao_Hua3|~XU_Panpan1|~Qian_HE3,Jinshu Chen|Bingchuan Li|Miao Hua|XU Panpan|Qian HE,few-shot|customized effects|paired data|diffusion models,"Existing solutions to image editing tasks suffer from several issues. Though achieving remarkably satisfying generated results, some supervised methods require huge amounts of paired training data, which greatly limits their usages. The other unsupervised methods take full advantage of large-scale pre-trained priors, thus being strictly restricted to the domains where the priors are trained on and behaving badly in out-of-distribution cases. The task we focus on is how to enable the users to customize their desired effects through only few image pairs. In our proposed framework, a novel few-shot learning mechanism based on the directional transformations among samples is introduced and expands the learnable space exponentially. Adopting a diffusion model pipeline, we redesign the condition calculating modules in our model and apply several technical improvements. Experimental results demonstrate the capabilities of our method in various cases.",No,,Accept (Poster),,https://arxiv.org/abs/2405.12490,,1
17,EverLight: Indoor-Outdoor Editable HDR Lighting Estimation,~Mohammad_Reza_Karimi_Dastjerdi1|~Jonathan_Eisenmann1|~Yannick_Hold-Geoffroy2|~Jean-Francois_Lalonde2,Mohammad Reza Karimi Dastjerdi|Jonathan Eisenmann|Yannick Hold-Geoffroy|Jean-Francois Lalonde,lighting estimation; image-based lighting; high dynamic range,"Because of the diversity in lighting environments, existing illumination estimation techniques have been designed explicitly on indoor or outdoor environments. Methods have focused specifically on capturing accurate energy (e.g., through parametric lighting models), which emphasizes shading and strong cast shadows; or producing plausible texture (e.g., with GANs), which prioritizes plausible reflections. Approaches which provide editable lighting capabilities have been proposed, but these tend to be with simplified lighting models, offering limited realism. In this work, we propose to bridge the gap between these recent trends in the literature, and propose a method which combines a parametric light model with 360° panoramas, ready to use as HDRI in rendering engines. We leverage recent advances in GAN-based LDR panorama extrapolation from a regular image, which we extend to HDR using parametric spherical gaussians. To achieve this, we introduce a novel lighting co-modulation method that injects lighting-related features throughout the generator, tightly coupling the original or edited scene illumination within the panorama generation process. In our representation, users can easily edit light direction, intensity, number, etc. to impact shading while providing rich, complex reflections while seamlessly blending with the edits. Furthermore, our method encompasses indoor and outdoor environments, demonstrating state-of-the-art results even when compared to domain-specific methods.",Yes,ICCV23,Accept (Poster),,,,2
18,NeRFFaceSpeech: One-shot Audio-diven 3D Talking Head Synthesis via Generative Prior,~Gihoon_Kim1|~Kwanggyoon_Seo1|~Sihun_Cha1|~Junyong_Noh1,Gihoon Kim|Kwanggyoon Seo|Sihun Cha|Junyong Noh,One-shot|Audio-diven|3D Talking Head|NeRF|Generative Prior,"Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency  in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively.",No,,Accept (Poster),,https://arxiv.org/abs/2405.05749,https://rlgnswk.github.io/NeRFFaceSpeech_ProjectPage/,1
19,FlexEControl: Flexible and Efficient Multimodal Control for Text-to-Image Generation,~Xuehai_He1|~Jian_Zheng1|~Jacob_Zhiyuan_Fang1|~Robinson_Piramuthu1|~Mohit_Bansal2|~Vicente_Ordonez2|~Gunnar_A_Sigurdsson1|~Nanyun_Peng1|~Xin_Eric_Wang2,Xuehai He|Jian Zheng|Jacob Zhiyuan Fang|Robinson Piramuthu|Mohit Bansal|Vicente Ordonez|Gunnar A Sigurdsson|Nanyun Peng|Xin Eric Wang,Controllable Text-to-image Generation|Efficient Learning,"Controllable text-to-image (T2I) diffusion models generate images conditioned on both text prompts and semantic inputs of other modalities like edge maps. Nevertheless, current controllable T2I methods commonly face challenges related to efficiency and faithfulness, especially when conditioning on multiple inputs from either the same or diverse modalities. In this paper, we propose a novel Flexible and Efficient method, FlexEControl, for controllable T2I generation. At the core of FlexEControl is a unique weight decomposition strategy, which allows for streamlined integration of various input types. This approach not only enhances the faithfulness of the generated image to the control, but also significantly reduces the computational overhead typically associated with multimodal conditioning.
Our approach achieves a reduction of 41% in trainable parameters and 30% in memory usage compared with Uni-ControlNet. Moreover, it doubles data efficiency and can flexibly generate images under the guidance of multiple input conditions of various modalities.",No,,Accept (Poster),,https://arxiv.org/abs/2405.04834,,2
20,VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis,~Yumeng_Li2|~William_H._Beluch1|~Margret_Keuper1|~Dan_Zhang1|~Anna_Khoreva1,Yumeng Li|William H. Beluch|Margret Keuper|Dan Zhang|Anna Khoreva,Text-to-Video|Temporal Attention|Video Diffusion Model,"Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt. At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable. To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos. We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis based on the original single prompt leveraging LLMs, which gives accurate textual guidance to different visual states of longer videos, and 2) Temporal Attention Regularization (TAR) - a regularization technique to refine the temporal attention units of the pre-trained T2V diffusion models, which enables control over the video dynamics. We experimentally showcase the superiority of the proposed approach in generating longer, visually appealing videos over existing open-sourced T2V models.",No,,Accept (Poster),,https://drive.google.com/file/d/1Ea6TKR9y6O16_xWlUWpnwds67pDpIckD/view?usp=sharing,https://yumengli007.github.io/VSTAR/,1
21,Pix2Gif: Motion-Guided Diffusion for GIF Generation,~Hitesh_Kandala1|~Jianfeng_Gao1|~Jianwei_Yang1,Hitesh Kandala|Jianfeng Gao|Jianwei Yang,Diffusion|image-to-video generation|image-to-image translation,"We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in the teaser figure. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model -- it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16$\times$V100 GPUs.",No,,Accept (Poster),,,,2
22,LOVECon: Text-driven Training-free Long Video Editing with ControlNet,~Zhenyi_Liao1|~Zhijie_Deng1,Zhenyi Liao|Zhijie Deng,Video editing|Training-free|Diffusion models,"Leveraging pre-trained conditional diffusion models for video editing without further tuning has gained increasing attention due to its promise in film production, advertising, etc. 
Yet, seminal works in this line fall short in generation length, temporal coherence, or fidelity to the source video. 
This paper aims to bridge the gap, establishing a simple and effective baseline for training-free diffusion model-based long video editing. 
As suggested by prior arts, we build the pipeline upon ControlNet, which excels at various image editing tasks based on text prompts. 
To break down the length constraints caused by limited computational memory, we split the long video into consecutive windows and develop a novel cross-window attention mechanism to ensure the consistency of global style and maximize the smoothness among windows. 
To achieve more accurate control, we extract the information from the source video via DDIM inversion and integrate the outcomes into the latent states of the generations. 
We also incorporate a video frame interpolation model to mitigate the frame-level flickering issue. 
Extensive empirical studies verify the superior efficacy of our method over competing baselines across scenarios, including the replacement of the attributes of foreground objects, style transfer, and background replacement. 
Besides, our method manages to edit videos comprising hundreds of frames according to user requirements.",No,,Accept (Poster),,https://arxiv.org/abs/2310.09711,https://github.com/zhijie-group/LOVECon,1
23,LoopDraw: a Loop-Based Autoregressive Model for Shape Synthesis and Editing,~Nam_Anh_Dinh1|~Haochen_Wang2|~Greg_Shakhnarovich1|~Rana_Hanocka1,Nam Anh Dinh|Haochen Wang|Greg Shakhnarovich|Rana Hanocka,loops|3D|modeling|editing|autoregressive|sequence,"There is no settled universal 3D representation for geometry with many alternatives such as point clouds, meshes, implicit functions, and voxels to name a few. In this work, we present a new, compelling alternative for representing shapes using a sequence of cross-sectional closed loops. The loops across all planes form an organizational hierarchy which we leverage for autoregressive shape synthesis and editing. Loops are a non-local description of the underlying shape, as simple loop manipulations (such as shifts) result in significant structural changes to the geometry. This is in contrast to manipulating local primitives such as points in a point cloud or a triangle in a triangle mesh. We further demonstrate that loops are intuitive and natural primitive for analyzing and editing shapes, both computationally and for users.",No,,Accept (Poster),,https://arxiv.org/abs/2212.04981,https://threedle.github.io/LoopDraw,2
25,Posterior Distillation Sampling,~Juil_Koo1|~Chanho_Park2|~Minhyuk_Sung1,Juil Koo|Chanho Park|Minhyuk Sung,Diffusion model|NeRF editing|3D Gaussian Splatting editing|SVG editing|SDS,"We introduce Posterior Distillation Sampling (PDS), a novel optimization method for parametric image editing based on diffusion models. Existing optimization-based methods, which leverage the powerful 2D prior of diffusion models to handle various parametric images, have mainly focused on generation. Unlike generation, editing requires a balance between conforming to the target attribute and preserving the identity of the source content. Recent 2D image editing methods have achieved this balance by leveraging the stochastic latent encoded in the generative process of diffusion models. To extend the editing capabilities of diffusion models shown in pixel space to parameter space, we reformulate the 2D image editing method into an optimization form named PDS. PDS matches the stochastic latents of the source and the target, enabling the sampling of targets in diverse parameter spaces that align with a desired attribute while maintaining the source's identity. We demonstrate that this optimization resembles running a generative process with the target attribute, but aligning this process with the trajectory of the source's generative process. Extensive editing results in Neural Radiance Fields and Scalable Vector Graphics representations demonstrate that PDS is capable of sampling targets to fulfill the aforementioned balance across various parameter spaces.",Yes,CVPR24,Accept (Poster),,https://arxiv.org/abs/2311.13831,https://posterior-distillation-sampling.github.io/,1
28,Text Prompting for Multi-Concept Video Customization by Autoregressive Generation,~Divya_Kothandaraman1|~Kihyuk_Sohn1|~Ruben_Villegas1|~Paul_Voigtlaender1|~Dinesh_Manocha3|~Mohammad_Babaeizadeh1,Divya Kothandaraman|Kihyuk Sohn|Ruben Villegas|Paul Voigtlaender|Dinesh Manocha|Mohammad Babaeizadeh,multi-concept video customization|prompting|autoregressive generation;,"We present a method for multi-concept customization of pretrained text-to-video (T2V) models. Intuitively, the multi-concept customized video can be derived from the (non-linear) intersection of the video manifolds of the individual concepts, which is not straightforward to find. We hypothesize that sequential and controlled walking towards the intersection of the video manifolds, directed by text prompting, leads to the solution. To do so, we generate the various concepts and their corresponding interactions, sequentially, in an autoregressive manner. Our method can generate videos of multiple custom concepts (subjects, action and background) such as a teddy bear running towards a brown teapot, a dog playing violin and a teddy bear swimming in the ocean. We quantitatively evaluate our method using videoCLIP and DINO scores, in addition to human evaluation.",No,,Accept (Poster),,http://arxiv.org/abs/2405.13951,https://github.com/divyakraman/MultiConceptVideo2024,2
29,The Lost Melody: Empirical Observations on Text-to-Video Generation From A Storytelling Perspective,~Andrew_Shin1|~Yusuke_Mori1|~Kunitake_Kaneko1,Andrew Shin|Yusuke Mori|Kunitake Kaneko,text-to-video generation|generative models|story evaluation,"Text-to-video generation task has witnessed a notable progress, with the generated outcomes reflecting the text prompts with high fidelity and impressive visual qualities. However, current text-to-video generation models are invariably focused on conveying the visual elements of a single scene, and have so far been indifferent to another important potential of the medium, namely a storytelling. In this paper, we examine text-to-video generation from a storytelling perspective, which has been hardly investigated, and make empirical remarks that spotlight the limitations of current text-to-video generation scheme. We also propose an evaluation framework for storytelling aspects of videos, and discuss the potential future directions.",No,,Accept (Poster),,https://arxiv.org/abs/2405.08720,,1
30,Towards Safer AI Content Creation by Immunizing Text-to-image Models,~Amber_Yijia_Zheng1|~Raymond_A._Yeh1,Amber Yijia Zheng|Raymond A. Yeh,DreamBooth|Textual-Inversion|Diffusion model,"Advancements in open-sourced text-to-image models and fine-tuning methods have led to the increasing risk of malicious adaptation, i.e., fine-tuning to generate harmful/unauthorized content. Recent works, e.g., Glaze or MIST, have developed data-poisoning techniques which protect the data against adaptation methods. In this work, we consider an alternative paradigm for protection. We propose to ``immunize'' the model by learning model parameters that are difficult for the adaptation methods when fine-tuning malicious content; in short IMMA. Specifically, IMMA should be applied before the release of the model weights to mitigate these risks. Empirical results show IMMA's effectiveness against malicious adaptations, including mimicking the artistic style and learning of inappropriate/unauthorized content, over three adaptation methods: LoRA, Textual-Inversion, and DreamBooth.",No,,Accept (Poster),,,,2
31,Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap,~Dejia_Xu1|~Xingqian_Xu2|~Wenyan_Cong1|~Humphrey_Shi1|~Zhangyang_Wang1,Dejia Xu|Xingqian Xu|Wenyan Cong|Humphrey Shi|Zhangyang Wang,image inpainting|diffusion model,"Have you ever imagined how it would look if we placed new objects into paintings? For example, what would it look like if we placed a basketball into Claude Monet's ''Water Lilies, Evening Effect''?
We propose Reference-based Painterly Inpainting, a novel task that crosses the wild reference domain gap and implants novel objects into artworks. Although previous works have examined reference-based inpainting, they are not designed for large domain discrepancies between the target and the reference, such as inpainting an artistic image using a photorealistic reference. This paper proposes a novel diffusion framework, dubbed RefPaint, to ``inpaint more wildly'' by taking such references with large domain gaps. Built with an image-conditioned diffusion model, we introduce a ladder-side branch and a masked fusion mechanism to work with the inpainting mask. By decomposing the CLIP image embeddings at inference time, one can manipulate the strength of semantic and style information with ease. Experiments demonstrate that our proposed RefPaint framework produces significantly better results than existing methods. Our method enables creative painterly image inpainting with reference objects that would otherwise be difficult to achieve.",No,,Accept (Poster),,,,1
32,Spatial Steerability of GANs via Self-Supervision from Discriminator,~Jianyuan_Wang2|~Lalit_Bhagat1|~Ceyuan_Yang2|~Yinghao_Xu1|~Yujun_Shen1|~Hongdong_Li1|~Bolei_Zhou5,Jianyuan Wang|Lalit Bhagat|Ceyuan Yang|Yinghao Xu|Yujun Shen|Hongdong Li|Bolei Zhou,Generative models|spatial editing|interpretability,"In this work, we propose a self-supervised approach to improve the spatial steerability of GANs without searching for steerable directions in the latent space or requiring extra annotations. Specifically, we design randomly sampled Gaussian heatmaps to be encoded into the intermediate layers of generative models as spatial inductive bias. Along with training the GAN model from scratch, these heatmaps are being aligned with the emerging attention of the GAN's discriminator in a self-supervised learning manner. During inference, users can interact with the spatial heatmaps intuitively, enabling them to edit the output image by adjusting the scene layout, moving, or removing objects. Moreover, we incorporate DragGAN into our framework, which facilitates fine-grained manipulation within a reasonable time and supports a coarse-to-fine editing process. Extensive experiments show that the proposed method not only enables spatial editing over human faces, animal faces, outdoor scenes, and complicated multi-object indoor scenes but also brings improvement in synthesis quality.",Yes,CVPR22,Accept (Poster),,,,2
33,Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering,~Kim_Youwang1|~Tae-Hyun_Oh3|~Gerard_Pons-Moll2,Kim Youwang|Tae-Hyun Oh|Gerard Pons-Moll,Text-driven texture synthesis|Neural re-parameterization|Physically-Based Rendering,"We present Paint-it, a text-driven high-fidelity texture map synthesis method for 3D meshes via neural re-parameterized texture optimization. Paint-it synthesizes texture maps from a text description by synthesis-through-optimization, exploiting the Score-Distillation Sampling (SDS). We observe that directly applying SDS yields undesirable texture quality due to its noisy gradients. We reveal the importance of texture parameterization when using SDS. Specifically, we propose Deep Convolutional Physically-Based Rendering (DC-PBR) parameterization, which re-parameterizes the physically-based rendering (PBR) texture maps with randomly initialized convolution-based neural kernels, instead of a standard pixel-based parameterization. We show that DC-PBR inherently schedules the optimization curriculum according to texture frequency and naturally filters out the noisy signals from SDS. In experiments, Paint-it obtains remarkable quality PBR texture maps within 15 min., given only a text description. We demonstrate the generalizability and practicality of Paint-it by synthesizing high-quality texture maps for large-scale mesh datasets and showing test-time applications such as relighting and material control using a popular graphics engine. Project page: https://kim-youwang.github.io/paint-it",Yes,CVPR24,Accept (Poster),,https://arxiv.org/abs/2312.11360,https://kim-youwang.github.io/paint-it,1
34,InstructRL4Pix: Training Diffusion for Image Editing by Reinforcement Learning,~Tiancheng_Li4|~Jinxiu_Liu1|~Chen_Huajun1|~Qi_Liu19,Tiancheng Li|Jinxiu Liu|Chen Huajun|Qi Liu,image editing， diffusion model， reinforcement learning,"Instruction-based image editing has made great process in using natural human language to manipulate the visual content of images. However, existing models are limited by the quality of the dataset and cannot accurately localize editing regions in images with complex object relationships. In this paper, we propose Reinforcement Learning Guided Image Editing Method(InstructRL4Pix) to train a diffusion model to generate images that are guided by the attention maps of the target object. Our method maximizes the output of the reward model by calculating the distance between attention maps as a reward function and fine-tuning the diffusion model using proximal policy optimization (PPO). We evaluate our model in object insertion, removal, replacement, and transformation. Experimental results show that InstructRL4Pix breaks through the limitations of traditional datasets and uses unsupervised learning to optimize editing goals and achieve accurate image editing based on natural human commands.",No,,Accept (Poster),,,,2
35,Seamless Human Motion Composition with Blended Positional Encodings,~German_Barquero1|~Sergio_Escalera1|~Cristina_Palmero1,German Barquero|Sergio Escalera|Cristina Palmero,human motion|motion composition|motion generation|text-to-motion|temporal composition|diffusion|generative ai|blended positional encodings|bpe|relative encodings,"Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.",Yes,CVPR24,Accept (Poster),,https://arxiv.org/abs/2402.15509,https://barquerogerman.github.io/FlowMDM/,1
36,CustomText: Customized Textual Image Generation using Diffusion Models,~Shubham_Paliwal1|~Arushi_Jain3|~Monika_Sharma2|~Vikram_Jamwal1|~Lovekesh_Vig1,Shubham Paliwal|Arushi Jain|Monika Sharma|Vikram Jamwal|Lovekesh Vig,Image Editing|Diffusion Models|ControlNet|Consistency Decoder|Textual Image,"Textual image generation spans diverse fields like advertising, education, product packaging, social media, information visualization, and branding. Despite recent strides in language-guided image synthesis using diffusion models, current models excel in image generation but struggle with accurate text rendering and offer limited control over font attributes. In this paper, we aim to enhance the synthesis of high-quality images with precise text customization, thereby contributing to the advancement of image generation models. We call our proposed method CustomText. Our implementation leverages a pre-trained TextDiffuser model to enable control over font color, background, and types. Additionally, to address the challenge of accurately rendering small sized fonts, we train the ControlNet model for a consistency decoder, significantly enhancing text-generation performance. We assess the performance of CustomText in comparison to previous methods of textual image generation, showcasing superior results.",No,,Accept (Poster),,https://arxiv.org/abs/2405.12531,,2
38,The Curious Case of End Token: A Zero-Shot Disentangled Image Editing using CLIP,~Hidir_Yesiltepe1|~Yusuf_Dalva1|~Pinar_Yanardag1,Hidir Yesiltepe|Yusuf Dalva|Pinar Yanardag,diffusion|editing|clip,"Diffusion models have become prominent in creating high-quality images. However, unlike GAN models celebrated for their ability to edit images in a disentangled manner, diffusion-based text-to-image models struggle to achieve the same level of precise attribute manipulation without compromising image coherence. In this paper, CLIP which is often used in popular text-to-image diffusion models such as Stable Diffusion is capable of performing disentangled editing in a zero-shot manner. Through both qualitative and quantitative comparisons with state-of-the-art editing methods, we show that our approach yields competitive results. This insight may open opportunities for applying this method to various tasks, including image and video editing, providing a lightweight and efficient approach for disentangled editing.",No,,Accept (Poster),,,,2
39,SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation,~Yuxuan_Zhang3|~Yiren_Song1|~Jiaming_Liu7|~Rui_Wang33|~Jinpeng_Yu1|~Hao_Tang6|~Huaxia_Li1|~Xu_Tang1|~Yao_Hu1|~Han_SJTU_Pan1|~Zhongliang_Jing1,Yuxuan Zhang|Yiren Song|Jiaming Liu|Rui Wang|Jinpeng Yu|Hao Tang|Huaxia Li|Xu Tang|Yao Hu|Han SJTU Pan|Zhongliang Jing,subject-driven image generation|diffusion,"Recent advancements in subject-driven image generation have led to zero-shot generation, yet precise selection and focus on crucial subject representations remain challenging. Addressing this, we introduce the SSR-Encoder, a novel architecture designed for selectively capturing any subject from single or multiple reference images. It responds to various query modalities including text and masks, without necessitating test-time fine-tuning. The SSR-Encoder combines a Token-to-Patch Aligner that aligns query inputs with image patches and a Detail-Preserving Subject Encoder for extracting and preserving fine features of the subjects, thereby generating subject embeddings. These embeddings, used in conjunction with original text embeddings, condition the generation process. Characterized by its model generalizability and efficiency, the SSR-Encoder adapts to a range of custom models and control modules. Enhanced by the Embedding Consistency Regularization Loss for improved training, our extensive experiments demonstrate its effectiveness in versatile and high-quality image generation, indicating its broad applicability.",Yes,CVPR24,Accept (Poster),,,,1
41,CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers,~Andrew_Marmon1|~Grant_Schindler3|~Jose_Lezama1|~Dan_Kondratyuk1|~Bryan_Seybold1|~Irfan_Essa1,Andrew Marmon|Grant Schindler|Jose Lezama|Dan Kondratyuk|Bryan Seybold|Irfan Essa,Generative Video|3D Cameras,"We extend multimodal transformers to include 3D camera motion as a conditioning signal for the task of video generation. Generative video models are becoming increasingly powerful, thus focusing research efforts on methods of controlling the output of such models. We propose to add virtual 3D camera controls to generative video methods by conditioning generated video on an encoding of three-dimensional camera movement over the course of the generated video. Results demonstrate that we are (1) able to successfully control the camera during video generation, starting from a single frame and a camera signal, and (2) we demonstrate the accuracy of the generated 3D camera paths using traditional computer vision methods.",No,,Accept (Poster),,https://arxiv.org/abs/2405.13195,,1
42,ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models,~Jeong-gi_Kwak1|~Erqun_Dong1|~Yuhe_Jin1|~Hanseok_Ko1|~Shweta_Mahajan1|~Kwang_Moo_Yi1,Jeong-gi Kwak|Erqun Dong|Yuhe Jin|Hanseok Ko|Shweta Mahajan|Kwang Moo Yi,Novel-view synthesis|Diffusion models|Video diffusion,"Generating novel views of an object from a single image is a challenging task. It requires an understanding of the underlying 3D structure of the object from an image and rendering high-quality, spatially consistent new views. While recent methods for view synthesis based on diffusion have shown great progress, achieving consistency among various view estimates and at the same time abiding by the desired camera pose remains a critical problem yet to be solved. In this work, we demonstrate a strikingly simple method, where we utilize a pre-trained video diffusion model to solve this problem. Our key idea is that synthesizing a novel view could be reformulated as synthesizing a video of a camera going around the object of interest---a scanning video---which then allows us to leverage the powerful priors that a video diffusion model would have learned. Thus, to perform novel-view synthesis, we create a smooth camera trajectory to the target view that we wish to render, and denoise using both a view-conditioned diffusion model and a video diffusion model. By doing so, we obtain a highly consistent novel view synthesis, outperforming the state of the art.",Yes,CVPR24,Accept (Poster),,https://arxiv.org/abs/2312.01305/,https://ubc-vision.github.io/vivid123/,2
43,DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision,~Lu_Ling1|~Yichen_Sheng1|~Zhi_Tu1|~Wentian_Zhao3|~Cheng_Xin2|~Kun_Wan1|~Lantao_Yu3|~Qianyu_Guo3|~Zixun_Yu1|~Yawen_Lu2|~Xuanmao_Li1|~Xingpeng_Sun1|~Rohan_Ashok1|~Aniruddha_Mukherjee1|~Hao_Kang2|~Xiangrui_Kong1|~Gang_Hua3|~Tianyi_Zhang7|~Bedrich_Benes1|~Aniket_Bera1,Lu Ling|Yichen Sheng|Zhi Tu|Wentian Zhao|Cheng Xin|Kun Wan|Lantao Yu|Qianyu Guo|Zixun Yu|Yawen Lu|Xuanmao Li|Xingpeng Sun|Rohan Ashok|Aniruddha Mukherjee|Hao Kang|Xiangrui Kong|Gang Hua|Tianyi Zhang|Bedrich Benes|Aniket Bera,Scene Dataset|3D Representation|Foundation Model|Neural Radiance Field,"We have witnessed significant progress in deep learning-based 3D vision, ranging from neural radiance field (NeRF) based 3D representation learning to applications in novel view synthesis (NVS). However, existing scene-level datasets for deep learning-based 3D vision, limited to either synthetic environments or a narrow selection of real-world scenes, are quite insufficient. This insufficiency not only hinders a comprehensive benchmark of existing methods but also caps what could be explored in deep learning-based 3D analysis. To address this critical gap, we present DL3DV-10K, a large-scale scene dataset featuring 51.2 million frames from 10,510 videos captured from 65 types of point-of-interest (POI) locations, covering both bounded and unbounded scenes, with different levels of reflection, transparency, and lighting. We conducted a comprehensive benchmark of recent NVS methods on DL3DV-10K, which revealed valuable insights for future research in NVS. In addition, we have obtained encouraging results in a pilot study to learn generalizable NeRF from DL3DV-10K, which manifests the necessity of a large-scale scene-level dataset to forge a path toward a foundation model for learning 3D representation. Our DL3DV-10K dataset, benchmark results, and models will be publicly accessible.",Yes,CVPR24,Accept (Poster),,https://arxiv.org/pdf/2312.16256,https://arxiv.org/pdf/2312.16256,1
44,Temporally Consistent Object Editing in Videos using Extended Attention,~AmirHossein_Zamani1|~Amir_Aghdam1|~Tiberiu_Popa1|~Eugene_Belilovsky1,AmirHossein Zamani|Amir Aghdam|Tiberiu Popa|Eugene Belilovsky,Video editing|inpainting diffusion model|temporal consistency|extended attention modules|video retargeting,"Image generation and editing have seen a great deal of advancements with the rise of large-scale diffusion models that allow user control of different modalities such as text, mask, depth maps, etc. However, controlled editing of videos still lags behind. Prior work in this area has focused on using 2D diffusion models to globally change the style of an existing video. On the other hand in many practical applications editing localized parts of the video is critical. In this work, we propose a method to edit videos using a pre-trained inpainting image diffusion model. We systematically redesign the forward path of the model by replacing the self-attention modules with an extended version of attention modules that creates frame-level dependencies. In this way, we ensure that the edited information will be consistent across all the video frames no matter what the shape and position of the masked area is. We qualitatively compare our results with state-of-the-art in terms of accuracy on several video editing tasks like object retargeting, object replacement, and object removal tasks. Simulations demonstrate the superior performance of the proposed strategy.",No,,Accept (Poster),,,,2
45,ElasticDiffusion: Training-free Arbitrary Size Image Generation,~Moayed_Haji-Ali1|~Guha_Balakrishnan1|~Vicente_Ordonez2,Moayed Haji-Ali|Guha Balakrishnan|Vicente Ordonez,Diffusion models|Image Generation|Text2Image|Higher-resolution|Arbitrary-size,"Diffusion models have revolutionized image generation in recent years, yet they are still limited to a few sizes and aspect ratios. We propose ElasticDiffusion, a novel training-free decoding method that enables pretrained text-to-image diffusion models to generate images with various sizes. ElasticDiffusion attempts to decouple the generation trajectory of a pretrained model into local and global signals. The local signal controls low-level pixel information and can be estimated on local patches, while the global signal is used to maintain overall structural consistency and is estimated with a reference image. Our experiments on two datasets: CelebA-HQ (faces), and LAION-COCO (objects/indoor/outdoor scenes) demonstrate higher perceptual quality across aspect ratios compared to MultiDiffusion and the standard decoding strategy for Stable Diffusion. We test our method on CelebA-HQ (faces) and LAION-COCO (objects/indoor/outdoor scenes). Our experiments and qualitative results show superior image coherence quality across aspect ratios compared to MultiDiffusion and the standard decoding strategy of Stable Diffusion.",Yes,CVPR24,Accept (Poster),,,,1
46,ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing,~Alec_Helbling1|~Seongmin_Lee2|~Duen_Horng_Chau1,Alec Helbling|Seongmin Lee|Duen Horng Chau,Diffusion|LLM|Multi-modal,"Recently, researchers have proposed powerful systems for generating and manipulating images using natural language instructions. However, it is difficult to precisely specify many common classes of image transformations with text alone. For example, a user may wish to change the location and breed of a particular dog in an image with several similar dogs. This task is quite difficult with natural language alone, and would require a user to write a laboriously complex prompt that both disambiguates the target dog and describes the destination. We propose ClickDiffusion, a system for precise image manipulation and generation that combines natural language instructions with visual feedback provided by the user through a direct manipulation interface. We demonstrate that by serializing both an image and a multi-modal instruction into a textual representation it is possible to leverage LLMs to perform precise transformations of the layout and appearance of an image.",No,,Accept (Poster),,https://arxiv.org/abs/2404.04376,https://github.com/poloclub/ClickDiffusion/tree/main,2
48,EraseDraw: Learning to Insert Objects by Erasing Them from Images,~Alper_Canberk1|~Maksym_Bondarenko1|~Ege_Ozguroglu1|~Ruoshi_Liu2|~Carl_Vondrick2,Alper Canberk|Maksym Bondarenko|Ege Ozguroglu|Ruoshi Liu|Carl Vondrick,content creation|image editing|object insertion|dataset creation,"Creative processes like painting involve creating different components of an image one by one. Can we build a computational model to perform this task? Prior works often fail by making global changes to the image, inserting objects in unrealistic spatial locations, and generating inaccurate lighting details. We observe that while state-of-the-art models perform poorly on object insertion, they can remove objects and erase the background in natural images very well. Inverting the direction of object removal, we obtain high-quality data for learning to insert objects that are spatially, physically, and optically consistent with the surroundings. With this scalable automatic data generation pipeline, we create a dataset for learning object insertion, which is used to train our text-conditioned diffusion model. Qualitative and quantitative experiments have shown that our model achieves state-of-the-art results in object insertion, particularly for in-the-wild images. We show compelling results on diverse insertion prompts and images across various domains. In addition, we pioneer iterative image generation by combining our insertion model with beam search guided by CLIP.",No,,Accept (Poster),,,,2
50,ICE-G: Image Conditional Editing of 3D Gaussian Splats,~Vishnu_Jaganathan1|~Hanyun_Huang1|~Muhammad_Zubair_Irshad1|~Varun_Jampani2|~Amit_Raj1|~Zsolt_Kira1,Vishnu Jaganathan|Hanyun Huang|Muhammad Zubair Irshad|Varun Jampani|Amit Raj|Zsolt Kira,Gaussian Splatting|NeRFs|3D editing|DINO,"Recently many techniques have emerged to create high quality 3D assets and scenes. When it comes to editing of these objects, however, existing approaches are either slow, compromise on quality, or do not provide enough customization. We introduce a novel approach to quickly edit a 3D model from a single reference view. Our technique first segments the edit image, and then matches semantically corresponding regions across chosen segmented dataset views using DINO features. A color or texture change from a particular region of the edit image can then be applied to other views automatically in a semantically sensible manner. These edited views act as an updated dataset to further train and re-style the 3D scene. The end-result is therefore an edited 3D model. Our framework enables a wide variety of editing tasks such as manual local edits, correspondence based style transfer from any example image, and a combination of different styles from multiple example images. We use Gaussian Splats as our primary 3D representation due to their speed and ease of local editing, but our technique works for other methods such as NeRFs as well. We show through multiple examples that our method produces higher quality results while offering fine grained control of editing.",No,,Accept (Poster),,,https://ice-gaussian.github.io/,1
53,TriLoRA: Integrating SVD for Advanced Style Personalization in Text-to-Image,~Chengcheng_Feng1|~mu_he2|~Xiaofang_Zhao1|~haojie_yin1|~Qiuyu_Tian1|~Tang_Hongwei1|~Xing_Qiang_Wei2,Chengcheng Feng|mu he|Xiaofang Zhao|haojie yin|Qiuyu Tian|Tang Hongwei|Xing Qiang Wei,Diffusion Model|Stable Diffusion|Fine-tuning|LoRA,"As deep learning technology continues to advance, image generation models, especially models like Stable Diffusion, are finding increasingly widespread application in visual arts creation. However, these models often face challenges such as overfitting, lack of stability in generated results, and difficulties in accurately capturing the features desired by creators during the fine-tuning process. In response to these challenges, we propose an innovative method that integrates Singular Value Decomposition (SVD) into the Low-Rank Adaptation (LoRA) parameter update strategy, aimed at enhancing the fine-tuning efficiency and output quality of image generation models. By incorporating SVD within the LoRA framework, our method not only effectively reduces the risk of overfitting but also enhances the stability of model outputs, and captures subtle, creator-desired feature adjustments more accurately. We evaluated our method on multiple datasets, and the results show that, compared to traditional fine-tuning methods, our approach significantly improves the model's generalization ability and creative flexibility while maintaining the quality of generation. Moreover, this method maintains LoRA's excellent performance under resource-constrained conditions, allowing for significant improvements in image generation quality without sacrificing the original efficiency and resource advantages.",No,,Accept (Poster),,,,1
54,LEAST: Local text-conditioned image style transfer,~Silky_Singh1|~Surgan_Jandial2|~Simra_Shahid1|~Abhinav_Java1,Silky Singh|Surgan Jandial|Simra Shahid|Abhinav Java,style transfer|image editing,"Text-conditioned style transfer enables users to communicate their desired artistic styles through text descriptions, offering a new and expressive means of achieving stylization. In this work, we evaluate the text-conditioned image editing and style transfer techniques on their fine-grained understanding of user prompts for precise “local” style transfer. We find that current methods fail to accomplish localized style transfers effectively, either failing to localize style transfer to certain regions in the image, or distorting the content and structure of the input image. To this end, we carefully design an end-to-end pipeline that guarantees local style transfer according to users’ intent. Further, we substantiate the effectiveness of our approach through quantitative and qualitative analysis.",No,,Accept (Poster),,https://arxiv.org/abs/2405.16330,https://github.com/silky1708/local-style-transfer,2
55,My Body My Choice: Human-Centric Full-Body Anonymization,~Umur_A._Ciftci1|~Ali_Kemal_Tanriverdi1|~Ilke_Demir3,Umur A. Ciftci|Ali Kemal Tanriverdi|Ilke Demir,guided generative AI|body anonymization|human generation|adversarial attack,"In an era of increasing privacy concerns for our online presence, we propose that the decision to appear in a piece of content should only belong to the owner of the body. Although some automatic approaches for full-body anonymization have been proposed, human-guided anonymization can adapt to various contexts, such as cultural norms, personal relations, esthetic concerns, and security issues. ""My Body My Choice"" (MBMC) enables physical and adversarial anonymization by removal and swapping approaches aimed for four tasks, designed by single or multi, ControlNet or GAN modules, combining several diffusion models. We evaluate anonymization on seven datasets; compare with SOTA inpainting and anonymization methods; evaluate by image, adversarial, and generative metrics; and conduct reidentification experiments.",No,,Accept (Poster),,,,1
