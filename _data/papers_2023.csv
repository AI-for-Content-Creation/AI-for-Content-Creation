openreview_number,title,authorids,authors,keywords,abstract,accepted_elsewhere,publication_conference_if_published,decision,award,arXiv,webpage,poster_session
3,SVS: Adversarial refinement for sparse novel view synthesis,~Violeta_Menendez_Gonzalez1|~Andrew_Gilbert1|graeme.phillipson@bbc.co.uk|stephen.jolly@bbc.co.uk|~Simon_Hadfield1,Violeta Menendez Gonzalez|Andrew Gilbert|Graeme Phillipson|Stephen Jolly|Simon Hadfield,novel view synthesis|neural radiance fields|adversarial training|sparse novel view,"This paper proposes Sparse View Synthesis. This is a view synthesis problem where the number of reference views is limited, and the baseline between target and reference view is significant. Under these conditions, current radiance field methods fail catastrophically due to inescapable artifacts such 3d floating blobs, blurring and structural duplication, whenever the number of reference views is limited, or the target view diverges significantly from the reference views. Advances in network architecture and loss regularisation are unable to satisfactorily remove these artifacts. The occlusions within the scene ensure that the true contents of these regions is simply not available to the model. In this work, we instead focus on hallucinating plausible scene contents within such regions. To this end we unify radiance field models with adversarial learning and perceptual losses. The resulting system provides up to 60% improvement in perceptual accuracy compared to current state-of-the-art radiance field models on this problem.",Yes,BMVC 2022,Accept (Poster),,https://arxiv.org/abs/2211.07301,https://bmvc2022.mpi-inf.mpg.de/886/,1
4,MetaDance: Few-shot Dancing Video Retargeting via Temporal-aware Meta-learning,~Yuying_Ge2|~Ruimao_Zhang1|~Yibing_Song1,Yuying Ge|Ruimao Zhang|Yibing Song,Few-shot Dancing Video Retargeting|Video Synthesis|Meta-learning,"Dancing video retargeting aims to synthesize a video that transfers the dance movements from a source video to a target person. Previous work need collect a several-minute-long video of a target person with thousands of frames to train a personalized model. However, the trained model can only generate videos of the same person. To address the limitations, recent work tackled few-shot dancing video retargeting, which learns to synthesize videos of unseen persons by leveraging a few frames of them. In practice, given a few frames of a person, these work simply regarded them as a batch of individual images without temporal correlations, thus generating temporally incoherent dancing videos of low visual quality. In this work, we model a few frames of a person as a series of dancing moves, where each move contains two consecutive frames, to extract the appearance patterns and the temporal dynamics of this person. We propose MetaDance, which utilizes temporal-aware meta-learning to optimize the initialization of a model through the synthesis of dancing moves, such that the meta-trained model can be efficiently tuned towards enhanced visual quality and strengthened temporal stability for unseen persons with a few frames. Extensive evaluations show large superiority of our method.",No,,Accept (Poster),,https://arxiv.org/abs/2201.04851,,2
5,Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models,~Jooyoung_Choi1|~Yunjey_Choi3|~Yunji_Kim1|~Junho_Kim3,Jooyoung Choi|Yunjey Choi|Yunji Kim|Junho Kim,Diffusion models|Image editing|Customization|Text-to-image,"Text-to-image diffusion models can generate diverse, high-fidelity images based on user-provided text prompts. Recent research has extended these models to support text-guided image editing. While text guidance is an intuitive editing interface for users, it often fails to ensure the precise concept conveyed by users. To address this issue, we propose Custom-Edit, in which we (i) customize a diffusion model with a few reference images and then (ii) perform text-guided editing. Our key discovery is that customizing only language-relevant parameters with augmented prompts improves reference similarity significantly while maintaining source similarity. Moreover, we provide our recipe for each customization and editing process. We compare popular customization methods and validate our findings on two editing methods using various datasets.",No,,Accept (Oral),,https://arxiv.org/abs/2305.15779,,2
6,SpaText: Spatio-Textual Representation for Controllable Image Generation ,~Omri_Avrahami1|~Thomas_F_Hayes1|~Oran_Gafni1|~Sonal_Gupta1|~Yaniv_Taigman1|~Devi_Parikh1|~Dani_Lischinski2|~Ohad_Fried1|~Xi_Yin3,Omri Avrahami|Thomas F Hayes|Oran Gafni|Sonal Gupta|Yaniv Taigman|Devi Parikh|Dani Lischinski|Ohad Fried|Xi Yin,,"Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText --- a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.",Yes,CVPR 2023,Accept (Poster),,https://arxiv.org/abs/2211.14305,https://omriavrahami.com/spatext/,1
7,The Nuts and Bolts of Adopting Transformer in GANs,~Rui_Xu2|~Xiangyu_Xu3|~Kai_Chen4|~Bolei_Zhou5|~Chen_Change_Loy2,Rui Xu|Xiangyu Xu|Kai Chen|Bolei Zhou|Chen Change Loy,GAN|Transformer,"Transformer becomes prevalent in computer vision, especially for high-level vision tasks. However, adopting Transformer in the generative adversarial network (GAN) framework is still an open yet challenging problem. In this paper, we conduct a comprehensive empirical study to investigate the properties of Transformer in GAN for high-fidelity image synthesis.  Our analysis highlights and reaffirms the importance of feature locality in image generation, although the merits of the locality are well known in the classification task. Perhaps more interestingly, we find the residual connections in self-attention layers harmful for learning Transformer-based discriminators and conditional generators. We carefully examine the influence and propose effective ways to mitigate the negative impacts. Our study leads to a new alternative design of Transformers in GAN, a convolutional neural network (CNN)-free generator termed as STrans-G, which achieves competitive results in both unconditional and conditional image generations. The Transformer-based discriminator, STrans-D, also significantly reduces its gap against the CNN-based discriminators.    ",No,,Accept (Poster),,https://arxiv.org/abs/2110.13107,,2
9,Can We Find Strong Lottery Tickets in Generative Models?,~Sangyeop_Yeo1|~Yoojin_Jang1|~Jy-yong_Sohn1|~Dongyoon_Han1|~Jaejun_Yoo1,Sangyeop Yeo|Yoojin Jang|Jy-yong Sohn|Dongyoon Han|Jaejun Yoo,,"Yes. In this paper, we investigate  \emph{strong lottery tickets} in generative models, the subnetworks that achieve good generative performance without any weight update. Neural network pruning is considered the main cornerstone of model compression for reducing the costs of computation and memory. Unfortunately, pruning a generative model has not been extensively explored, and all existing pruning algorithms suffer from excessive weight-training costs, performance degradation, limited generalizability, or complicated training. To address these problems, we propose to find a strong lottery ticket via moment-matching scores. Our experimental results show that the discovered subnetwork can perform similarly or better than the trained dense model even when only 10\% of the weights remain. To the best of our knowledge, we are the first to show the existence of strong lottery tickets in generative models and provide an algorithm to find it stably. Our code and supplementary materials are publicly available at https://lait-cvlab.github.io/SLT-in-Generative-Models/.",Yes,AAAI 2023,Accept (Poster),,https://arxiv.org/abs/2212.08311,,1
11,LPMM : Intuitive Pose Control for Neural Talking-Head Model via Landmark-Parameter Morphable Model ,~Kwangho_Lee1|~Patrick_Kwon2|~Myung_Ki_Lee1|~Namhyuk_Ahn1|~Junsoo_Lee1,Kwangho Lee|Patrick Kwon|Myung Ki Lee|Namhyuk Ahn|Junsoo Lee,generative model|video editing|neural talking head,"While current talking head models are capable of generating photorealistic talking head videos, they provide limited pose controllability.  Most methods require specific video sequences that should exactly contain the head pose desired, being far from user-friendly pose control.  Three-dimensional morphable models (3DMM) offer semantic pose control, but they fail to capture certain expressions. We present a novel method that utilizes parametric control of head orientation and facial expression over a pre-trained neural-talking head model. To enable this, we introduce a landmark-parameter morphable model (LPMM), which offers control over the facial landmark domain through a set of semantic parameters. Using LPMM, it is possible to adjust specific head pose factors, without distorting other facial attributes. The results show our approach provides intuitive rig-like control over neural talking head models, allowing both parameter and image-based inputs.",No,,Accept (Poster),,https://arxiv.org/abs/2305.10456,https://khlee369.github.io/LPMM/,2
12,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,~Jiaxin_Cheng1|~Xiao_Liang7|~Xingjian_Shi1|~Tong_He5|~Tianjun_Xiao1|~Mu_Li4,Jiaxin Cheng|Xiao Liang|Xingjian Shi|Tong He|Tianjun Xiao|Mu Li,Computer Vision|Image Generation|Diffusion Model|Layout-to-image,"Layout-to-image generation refers to the task of synthesizing photo-realistic images based on semantic layouts. In this paper, we propose LayoutDiffuse that adapts a foundational diffusion model pretrained on large-scale image or text-image datasets for layout-to-image generation. By adopting a novel neural adaptor based on layout attention and task-aware prompts, our method trains efficiently, generates images with both high perceptual quality and layout alignment, and needs less data.  Experiments on three datasets show that our method significantly outperforms other 10 generative models based on GANs, VQ-VAE, and diffusion models.",No,,Accept (Poster),,https://arxiv.org/abs/2302.08908,https://github.com/cplusx/layout_diffuse,2
14,Exploring Gradient-based Multi-directional Controls in GANs,~Zikun_Chen3|~Ruowei_Jiang1|~Brendan_Duke1|~Han_Zhao1|~Parham_Aarabi2,Zikun Chen|Ruowei Jiang|Brendan Duke|Han Zhao|Parham Aarabi,GAN|gradient information|latent space|disentanglement|multi-directional,"Generative Adversarial Networks (GANs) have been widely applied in modeling diverse image distributions.  However, despite its impressive applications, the structure of the latent space in GANs largely remains as a black-box, leaving its controllable generation an open problem, especially when spurious correlations between different semantic attributes exist in the image distributions.  To address this problem, previous methods typically learn linear directions or individual channels that control semantic attributes in the image space. However, they often suffer from imperfect disentanglement, or are unable to obtain multi-directional controls.   In this work, in light of the above challenges, we propose a novel approach that discovers nonlinear controls, which enables multi-directional manipulation as well as effective disentanglement, based on gradient information in the learned GAN latent space.   More specifically, we first learn interpolation directions by following the gradients from classification networks trained separately on the attributes, and then navigate the latent space by exclusively controlling channels activated for the target attribute in the learned directions. Empirically, with small training data, our approach is able to gain fine-grained controls over a diverse set of bi-directional and multi-directional attributes, and we showcase its ability to achieve disentanglement significantly better than state-of-the-art methods both qualitatively and quantitatively. ",Yes,ECCV2022,Accept (Poster),,https://arxiv.org/abs/2209.00698,,1
15,Reference-based Image Composition with Sketch via Structure-aware Diffusion Model,~Kangyeol_Kim1|~Sunghyun_Park2|~Junsoo_Lee1|~Jaegul_Choo1,Kangyeol Kim|Sunghyun Park|Junsoo Lee|Jaegul Choo,Generative model|Diffusion|Cartoon creation|Image manipulation,"Recent remarkable improvements in large-scale text-to-image generative models have shown promising results in generating high-fidelity images. To further enhance editability and enable fine-grained generation, we introduce a multi-input-conditioned image composition model that incorporates a sketch as a novel modal, alongside a reference image. Thanks to the edge-level controllability using sketches, our method enables a user to edit or complete an image sub-part with a desired structure (i.e., sketch) and content (i.e., reference image). Our framework fine-tunes a pre-trained diffusion model to complete missing regions using the reference image while maintaining sketch guidance. Albeit simple, this leads to wide opportunities to fulfill user needs for obtaining the in-demand images. Through extensive experiments, we demonstrate that our proposed method offers unique use cases for image manipulation, enabling user-driven modifications of arbitrary scenes.",No,,Accept (Poster),,https://arxiv.org/abs/2304.09748,https://github.com/kangyeolk/Paint-by-Sketch,2
18,3DAvatarGAN: Bridging Domains for Personalized Editable Avatars,~Rameen_Abdal1|~Hsin-Ying_Lee2|~Peihao_Zhu1|~Menglei_Chai1|~Aliaksandr_Siarohin1|~Peter_Wonka1|~Sergey_Tulyakov1,Rameen Abdal|Hsin-Ying Lee|Peihao Zhu|Menglei Chai|Aliaksandr Siarohin|Peter Wonka|Sergey Tulyakov,3D Avatars|3D GANs|Animation|Image Editing,"Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure. Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible. Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We, then, distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling---as a byproduct---personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions---for the first time---allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets.",Yes,CVPR 2023,Accept (Poster),,https://arxiv.org/abs/2301.02700,https://rameenabdal.github.io/3DAvatarGAN/,1
19,Putting People in Their Place: Affordance-Aware Human Insertion into Scenes,~Sumith_Kulal1|~Tim_Brooks1|~Alex_Aiken1|~Jiajun_Wu1|~Jimei_Yang1|~Jingwan_Lu1|~Alexei_A_Efros1|~Krishna_Kumar_Singh4,Sumith Kulal|Tim Brooks|Alex Aiken|Jiajun Wu|Jimei Yang|Jingwan Lu|Alexei A Efros|Krishna Kumar Singh,Image synthesis|generative models|human image synthesis|affordance perception|self-supervised models.,"We study the problem of inferring scene affordances by presenting a method for realistically inserting people into scenes. Given a scene image with a marked region and an image of a person, we insert the person into the scene while respecting the scene affordances. Our model can infer the set of realistic poses given the scene context, re-pose the reference person, and harmonize the composition. We set up the task in a self-supervised fashion by learning to re- pose humans in video clips. We train a large-scale diffusion model on a dataset of 2.4M video clips that produces diverse plausible poses while respecting the scene context. Given the learned human-scene composition, our model can also hallucinate realistic people and scenes when prompted without conditioning and also enables interactive editing. We conduct quantitative evaluation and show that our method synthesizes more realistic human appearance and more natural human-scene interactions when compared to prior work.",Yes,CVPR 2023,Accept (Poster),bestposter,https://arxiv.org/abs/2304.14406,https://sumith1896.github.io/affordance-insertion/,1
20,Balancing Reconstruction and Editing Quality of GAN Inversion for Real Image Editing with StyleGAN Prior Latent Space,~Kai_Katsumata1|~MinhDuc_Vo1|~Bei_Liu2|~Hideki_Nakayama1,Kai Katsumata|MinhDuc Vo|Bei Liu|Hideki Nakayama,GAN inversion,"The exploration of the latent space in StyleGANs and GAN inversion exemplify impressive real-world image editing, yet the trade-off between reconstruction quality and editing quality remains an open problem. In this study, we revisit StyleGANs' hyperspherical prior $\mathcal{Z}$ and $\mathcal{Z}^{+}$ and integrate them into seminal GAN inversion methods to improve editing quality. Besides faithful reconstruction, our extensions achieve sophisticated editing quality with the aid of the StyleGAN prior. We project the real images into the proposed space to obtain the inverted codes, by which we then move along $\mathcal{Z}^{+}$, enabling semantic editing without sacrificing image quality. Comprehensive experiments show that $\mathcal{Z}^{+}$ can replace the most commonly-used $\mathcal{W}$, $\mathcal{W}^{+}$, and $\mathcal{S}$ spaces while preserving reconstruction quality, resulting in reduced distortion of edited images.",No,,Accept (Poster),,https://arxiv.org/abs/2306.00241,,2
21,Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models,~Naoki_Matsunaga1|~Masato_Ishii1|~Akio_Hayakawa1|~Kenji_Suzuki4|~Takuya_Narihira2,Naoki Matsunaga|Masato Ishii|Akio Hayakawa|Kenji Suzuki|Takuya Narihira,Diffusion models|Image editing,"Our goal is to develop fine-grained real-image editing methods suitable for real-world applications. In this paper, we first summarize four requirements for these methods and propose a novel diffusion-based image editing framework with pixel-wise guidance that satisfies these requirements. Specifically, we train pixel-classifiers with a few annotated data and then infer the segmentation map of a target image. Users then manipulate the map to instruct how the image will be edited. We utilize a pre-trained diffusion model to generate edited images aligned with the user's intention with pixel-wise guidance. The effective combination of proposed guidance and other techniques enables highly controllable editing with preserving the outside of the edited area, which results in meeting our requirements. The experimental results demonstrate that our proposal outperforms the GAN-based method for editing quality and speed. ",No,,Accept (Oral),bestpaper,https://arxiv.org/abs/2212.02024,,2
24,'Tax-free' 3DMM Conditional Face Generation,~Nick_Huang1|~Zhiqiu_Yu1|~Xinjie_Yi1|~Yue_Wang28|~James_Tompkin3,Nick Huang|Zhiqiu Yu|Xinjie Yi|Yue Wang|James Tompkin,3D Morphable model|3DMM|StyleGAN|GAN|Face synthesis|Information theory|Disentanglement,"3DMM conditioned face generation has gained traction due to its well-defined controllability; however, the trade-off is lower sample quality: Previous works such as DiscoFaceGAN and 3D-FM GAN show a significant FID gap compared to the unconditional StyleGAN, suggesting that there is a quality tax to pay for controllability. In this paper, we challenge the assumption that quality and controllability cannot coexist. To pinpoint the previous issues, we mathematically formalize the problem of 3DMM conditioned face generation. Then, we devise simple solutions to the problem under our proposed framework. This results in a new model that effectively removes the quality tax between 3DMM conditioned face GANs and the unconditional StyleGAN.",No,,Accept (Oral),,https://arxiv.org/abs/2305.13460,,2
25,CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes,~Kim_Youwang1|jyeonkim@postech.ac.kr|~Tae-Hyun_Oh3,Kim Youwang|Kim Ji-Yeon|Tae-Hyun Oh,human avatar|text-to-3D|mesh animation|mesh stylization|text-driven manipulation,"We propose CLIP-Actor, a text-driven motion recommendation and neural mesh stylization system for human mesh animation. CLIP-Actor animates a 3D human mesh to conform to a text prompt by recommending a motion sequence and optimizing mesh style attributes. Prior work fails to generate plausible results when the pose of an artist designed mesh does not conform to the text from the beginning. Instead, we build a text-driven human motion recommendation system by leveraging a large-scale human motion dataset with language labels. Given a natural language prompt, CLIP-Actor suggests a text-conforming human motion in a coarse-to-fine manner. Then our novel neural style optimization detailizes and texturizes the recommended mesh sequence to conform to the prompt in a temporally-consistent and pose-agnostic manner. We further propose the spatio-temporal view augmentation and mask-weighted embedding attention, which stabilizes the optimization process by leveraging multi-frame human motion and rejecting poorly rendered views. We demonstrate that CLIP-Actor produces plausible and human-recognizable style 3D human mesh in motion with detailed geometry and texture solely from a natural language prompt. ",Yes,ECCV 2022,Accept (Poster),,https://arxiv.org/abs/2206.04382,https://clip-actor.github.io/,1
26,High-fidelity 3D Human Digitization from Single 2K Resolution Images,~Sang-Hun_Han1|~Min-Gyu_Park2|~Ju_Hong_Yoon2|~Ju-Mi_Kang1|~Young-Jae_Park1|~Hae-Gon_Jeon3,Sang-Hun Han|Min-Gyu Park|Ju Hong Yoon|Ju-Mi Kang|Young-Jae Park|Hae-Gon Jeon,Human Reconstruction|Surface Representation|3D from Single Image|Multi-view Dataset,"High-quality 3D human body reconstruction requires high-fidelity and large-scale training data and appropriate network design that effectively exploits the high-resolution input images. To tackle these problems, we construct a large-scale 3D human dataset and propose a simple yet effective 3D human digitization method. The proposed method separately recovers the global shape of a human and its details. The low-resolution depth network predicts the global structure from a low-resolution image, and the part-wise image-to-normal network predicts the details of the 3D human body structure. The high-resolution depth network merges the global 3D shape and the detailed structures to infer the high-resolution front and back side depth maps. Finally, an off-the-shelf mesh generator reconstructs the full 3D human model. In addition, we also provide 2,050 3D human models, including texture maps, 3D joints, and SMPL parameters for research purposes. In experiments, we demonstrate competitive performance over the recent works on various datasets.",Yes,CVPR 2023,Accept (Poster),,https://arxiv.org/abs/2303.15108,https://sanghunhan92.github.io/conference/2K2K/,1
28,Instance-Aware Image Completion,~Jinoh_Cho1|~Minguk_Kang1|~Vibhav_Vineet5|~Jaesik_Park3,Jinoh Cho|Minguk Kang|Vibhav Vineet|Jaesik Park,Image Inpainting|Image Completion,"Image completion is a task that aims to fill in the missing region of a masked image with plausible contents. However, existing image completion methods tend to fill in the missing region with the surrounding texture instead of hallucinating a visual instance that is suitable in accordance with the context of the scene. In this work, we propose a novel image completion model, dubbed ImComplete, that hallucinates the missing instance that harmonizes well with - and thus preserves - the original context. ImComplete first adopts a transformer architecture that considers the visible instances and the location of the missing region. Then, ImComplete completes the semantic segmentation masks within the missing region, providing pixel-level semantic and structural guidance. Finally, the image synthesis blocks generate photo-realistic content. We perform a comprehensive evaluation of the results in terms of visual quality (LPIPS and FID) and contextual preservation scores (CLIPscore and object detection accuracy) with COCO-panoptic and Visual Genome datasets. Experimental results show the superiority of ImComplete on various natural images.",No,,Accept (Poster),,https://arxiv.org/abs/2210.12350,,2
29,Context-Preserving Two-Stage Video Domain Translation for Portrait Stylization,~Doyeon_Kim1|~Eunji_Ko1|~Hyunsu_Kim1|~Yunji_Kim1|~Junho_Kim3|~Dongchan_Min1|~Junmo_Kim1|~Sung_Ju_Hwang1,Doyeon Kim|Eunji Ko|Hyunsu Kim|Yunji Kim|Junho Kim|Dongchan Min|Junmo Kim|Sung Ju Hwang,Generative Adversarial Networks|Style Transfer|Video Translation,"Portrait stylization, which translates a real human face image into an artistically stylized image, has attracted considerable interest and many prior works have shown impressive quality in recent years. However, despite their remarkable performances in the image-level translation tasks, prior methods show unsatisfactory results when they are applied to the video domain. To address the issue, we propose a novel two-stage video translation framework with an objective function which enforces a model to generate a temporally coherent stylized video while preserving context in the source video. Furthermore, our model runs in real-time with the latency of 0.011 seconds per frame and requires only 5.6M parameters, and thus is widely applicable to practical real-world applications.",No,,Accept (Poster),,https://arxiv.org/abs/2305.19135,,2
30,Visual prompt tuning for generative transfer learning,~Kihyuk_Sohn1|~Huiwen_Chang2|~Jose_Lezama1|~Luisa_Polania_Cabrera1|~Han_Zhang5|~Yuan_Hao1|~Irfan_Essa1|~Lu_Jiang1,Kihyuk Sohn|Huiwen Chang|Jose Lezama|Luisa Polania Cabrera|Han Zhang|Yuan Hao|Irfan Essa|Lu Jiang,,"Learning generative image models from various domains efficiently needs transferring knowledge from an image synthesis model trained on a large dataset.  We present a recipe for learning vision transformers by generative knowledge transfer.  We base our framework on generative vision transformers representing an image as a sequence of visual tokens with the autoregressive or non-autoregressive transformers.   To adapt to a new domain, we employ prompt tuning, which prepends learnable tokens called prompts to the image token sequence and introduces a new prompt design for our task. We study on a variety of visual domains with varying amounts of training images. We show the effectiveness of knowledge transfer and a significantly better image generation quality. Code is available at https://github.com/google-research/generative_transfer",Yes,CVPR 2023,Accept (Poster),,https://arxiv.org/abs/2210.00990,,1
31,Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment,~Kim_Sung-Bin1|~Arda_Senocak1|~Hyunwoo_Ha1|~Andrew_Owens1|~Tae-Hyun_Oh3,Kim Sung-Bin|Arda Senocak|Hyunwoo Ha|Andrew Owens|Tae-Hyun Oh,Multi-modal with audio,"How does audio describe the world around us? In this paper, we propose a method for generating an image of a scene from sound. Our method addresses the challenges of dealing with the large gaps that often exist between sight and sound. We design a model that works by scheduling the learning procedure of each model component to associate audio-visual modalities despite their information gaps. The key idea is to enrich the audio features with visual information by learning to align audio to visual latent space. We translate the input audio to visual features, then use a pre-trained generator to produce an image. To further improve the quality of our generated images, we use sound source localization to select the audio-visual pairs that have strong cross-modal correlations. We obtain substantially better results on the VEGAS and VGGSound datasets than prior approaches. We also show that we can control our modelâ€™s predictions by applying simple manipulations to the input waveform, or to the latent space.",Yes,CVPR 2023,Accept (Poster),,https://arxiv.org/abs/2303.17490,,1
33,Neural Sign Reenactor: Deep Photorealistic Sign Language Retargeting,~Christina_Ourania_Tze1|~Panagiotis_Filntisis1|ndimou@athenarc.gr|~Anastasios_Roussos1|~Petros_Maragos1,Christina Ourania Tze|Panagiotis Filntisis|Athanasia-Lida Dimou|Anastasios Roussos|Petros Maragos,,"In this paper, we introduce a neural rendering pipeline for transferring the facial expressions, head pose, and body movements of one person in a source video to another in a target video. We apply our method to the challenging case of Sign Language videos: given a source video of a sign language user, we can faithfully transfer the performed manual (e.g., handshape, palm orientation, movement, location) and non-manual (e.g., eye gaze, facial expressions, mouth patterns, head, and body movements) signs to a target video in a photo-realistic manner. We conduct detailed qualitative and quantitative evaluations and comparisons, which demonstrate the particularly promising and realistic results that we obtain as well as the advantages of our method over existing approaches.",No,,Accept (Poster),,https://arxiv.org/abs/2209.01470,,2
35,Paste and Harmonize via Denoising: Towards Controllable Exemplar-based Image Editing,~XIN_ZHANG12|~Jiaxian_Guo2|~Paul_Yoo1|~Yutaka_Matsuo1|~Yusuke_Iwasawa1,Xin Zhang|Jiaxian Guo|Paul Yoo|Yutaka Matsuo|Yusuke Iwasawa,Exemplar-based Image Editing|Diffusion Model|Harmonization.,"Text-to-Image generative models excel at creating high-quality images, but struggle with exemplar-guided image editing without compromising object identity, limiting practical applications. We propose \textit{Paste and Harmonize via Denoising}, a framework using pre-trained diffusion models for text-driven object transfer between images while preserving appearance and characteristics. The framework consists of two main steps: \textit{paste} and \textit{harmonize via denoising}. In the \textit{paste} step, an off-the-shelf segmentation model is utilized to localize objects in the exemplar image, and by pasting the object patches into the editing image, the task naturally transforms into an image harmonization task. We then introduce an \textit{image harmonization module} to guide the pre-trained diffusion model in blending the inserted object with the target image without altering its parameters, preserving concept composition and text-driven style transfer editing capabilities.  In the experiments, the qualitative comparisons with baselines demonstrate that our method achieves impressive performance in exemplar-based image editing on both training and in-the-wild images in exemplar-based image editing tasks. More qualitative results can be found at \url{https://sites.google.com/view/phd-demo-page}.",No,,Accept (Poster),,http://arxiv.org/abs/2306.07596,https://sites.google.com/view/phd-demo-page,2
37,Text-to-image Editing by Image Information Removal,~Zhongping_Zhang1|~Jian_Zheng1|~Jacob_Zhiyuan_Fang1|~Bryan_A._Plummer1,Zhongping Zhang|Jian Zheng|Jacob Zhiyuan Fang|Bryan A. Plummer,text-to-image editing|conditional diffusion model|unsupervised attribute removal,"Text-to-image editing controls and modifies specific content in the input image according to the textual descriptions. To leverage the strong capability of large-scale conditional diffusion models, recent methods typically edit images by finetuning pretrained generative models (e.g., Stable Diffusion or Imagen) and learning unique text embeddings for each input image (i.e., internal learning). However, there are two major challenges of this kind of method. First, finetuning pretrained models on a single image may lead to severe overfitting, which makes the model difficult to modify image content while preserving the irrelevant content of the original input image. Second, the inference time of internal learning methods is much longer than external learning methods, since the network parameters or feature embeddings need to be updated by hundreds of iterations. To solve these two challenges, we propose an unsupervised image information removal module and train our model on the image reconstruction task. The module includes two operations to erase both rigid and non-rigid image information. Our model eliminates the requirement to update parameters during inference, resulting in significantly faster image editing. We validate the effectiveness of our model on three public datasets, CUB, Outdoor Scenes, and COCO. The quantitative and qualitative results demonstrate that our approach achieves the best editability-fidelity tradeoff compared to current state-of-the-art methods.",No,,Accept (Oral),bestpaperrunnerup,https://arxiv.org/abs/2305.17489,,2
39,AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion,~SEUNGWOO_LEE2|~Chaerin_Kong1|~Donghyeon_Jeon1|~Nojun_Kwak1,Seungwoo Lee|Chaerin Kong|Donghyeon Jeon|Nojun Kwak,computer vision|generative models|diffusion models|multi-modal|text-to-video generation,"Recent advances in diffusion models have showcased promising results in the text-to-video (T2V) synthesis task. However, as these T2V models solely employ text as the guidance, they tend to struggle in modeling detailed temporal dynamics. In this paper, we introduce a novel T2V framework that additionally employ audio signals to control the temporal dynamics, empowering an off-the-shelf T2I diffusion to generate audio-aligned videos. We propose audio-based regional editing and signal smoothing to strike a good balance between the two contradicting desiderata of video synthesis, i.e., temporal flexibility and coherence. We empirically demonstrate the effectiveness of our method through experiments, and further present practical applications for contents creation.",No,,Accept (Poster),,https://arxiv.org/abs/2305.04001,,1
40,We never go out of Style: Motion Disentanglement by Subspace Decomposition of Latent Space,~Rishubh_Parihar1|~Raghav_Magazine1|~Piyush_Tiwari1|~Venkatesh_Babu_Radhakrishnan2,Rishubh Parihar|Raghav Magazine|Piyush Tiwari|Venkatesh Babu Radhakrishnan,Motion decomposition|Video Editing|GANs,"Real-world objects perform complex motions that involve multiple independent motion components. For example, while talking, a person continuously changes their expressions, head, and body pose. In this work, we propose a novel method to decompose motion in videos by using a pretrained image GAN model. We discover disentangled motion subspaces in the latent space of widely used style-based GAN models that are semantically meaningful and control a single explainable motion component. The proposed method uses only a few $(\approx10)$ ground truth video sequences to obtain such subspaces. We extensively evaluate the disentanglement properties of motion subspaces on face and car datasets, quantitatively and qualitatively. Further, we present results for multiple downstream tasks such as motion editing, and selective motion transfer, e.g. transferring only facial expressions without explicitly training for these tasks. ",No,,Accept (Poster),,https://arxiv.org/abs/2306.00559,,1
43,Discrete Predictor-Corrector Diffusion Models for Image Synthesis,~Jose_Lezama1|~Tim_Salimans1|~Lu_Jiang1|~Huiwen_Chang2|~Jonathan_Ho1|~Irfan_Essa1,Jose Lezama|Tim Salimans|Lu Jiang|Huiwen Chang|Jonathan Ho|Irfan Essa,,"We introduce Discrete Predictor-Corrector diffusion models (DPC), extending predictor-corrector samplers in Gaussian diffusion models to the discrete case. Predictor-corrector samplers are a class of samplers for diffusion models, which improve on ancestral samplers by correcting the sampling distribution of intermediate diffusion states using MCMC methods. In DPC, the Langevin corrector, which does not have a direct counterpart in discrete space, is replaced with a discrete MCMC transition defined by a learned corrector kernel. The corrector kernel is trained to make the correction steps achieve asymptotic convergence, in distribution, to the correct marginal of the intermediate diffusion states. Equipped with DPC, we revisit recent transformer-based non-autoregressive generative models through the lens of discrete diffusion, and find that DPC can alleviate the compounding decoding error due to the parallel sampling of visual tokens. Our experiments show that DPC improves upon existing discrete latent space models for class-conditional image generation on ImageNet, and outperforms continuous diffusion models and GANs, according to standard metrics and user preference studies. ",Yes,ICLR 2023,Accept (Poster),,https://openreview.net/forum?id=VM8batVBWvg,,1
44,BLT: Bidirectional Layout Transformer for Controllable Layout Generation,~Xiang_Kong1|~Lu_Jiang1|~Huiwen_Chang2|~Han_Zhang5|~Yuan_Hao1|~Haifeng_Gong2|~Irfan_Essa1,Xiang Kong|Lu Jiang|Huiwen Chang|Han Zhang|Yuan Hao|Haifeng Gong|Irfan Essa,,"Creating visual layouts is a critical step in graphic design. Automatic generation of such layouts is essential for scalable and diverse visual designs. To advance conditional layout generation, we introduce BLT, a bidirectional layout transformer. BLT differs from previous work on transformers in adopting non-autoregressive transformers. In training, BLT learns to predict the masked attributes by attending to surrounding attributes in two directions. During inference, BLT first generates a draft layout from the input and then iteratively refines it into a high-quality layout by masking out low-confident attributes. The masks generated in both training and inference are controlled by a new hierarchical sampling policy. We verify the proposed model on six benchmarks of diverse design tasks. Experimental results demonstrate two benefits compared to the state-of-the-art layout transformer models. First, our model empowers layout transformers to fulfill controllable layout generation. Second, it achieves up to 10x speedup in generating a layout at inference time than the layout transformer baseline. Code is released at https://github.com/google-research/google-research/tree/master/layout-blt",Yes,ECCV 2022,Accept (Poster),,https://arxiv.org/abs/2112.05112,https://shawnkx.github.io/blt,1
45,Matching-based Data Valuation for Generative Model,~Jiaxi_Yang2|~Wenlong_Deng1|~Benlin_Liu1|~Yangsibo_Huang2|~Xiaoxiao_Li1,Jiaxi Yang|Wenlong Deng|Benlin Liu|Yangsibo Huang|Xiaoxiao Li,Generative model|Data valuation,"Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for generation tasks. We have conducted extensive experiments to demonstrate the effectiveness of the proposed method. To the best of their knowledge, GMValuator is the first work that offers a training-free, post-hoc data valuation strategy for deep generative models.",No,,Accept (Poster),,https://arxiv.org/abs/2304.10701,,1
