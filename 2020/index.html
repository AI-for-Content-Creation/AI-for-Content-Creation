---
redirect_from: "aicc2020/"
---

<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <style>
    body {
        font-family: 'serif';
    }

    a:link {
        color: #C00404;
    }

    a:visited {
        color: #E21B23;
    }

    p {
        text-align: justify;
        -webkit-hyphens: auto;
            -moz-hyphens: auto;
        hyphens: auto;
    }

    li {
        margin: 0.5em 0em 0em 0em;
    }

    .brownbrown {
        color: #4E3629;
        highlight
    }

    .brownred {
        color: #C00404;
    }
    </style>


    <title>AI4CCW 2020</title>
  </head>
  <body>
    
    <div class="container brownbrown w-75" align="center">
        <hr>
        <h1>AI for Content Creation Workshop</h1>
        <h3>CVPR 2020</h3>
        <em><h3>June 15th</h3></em>
    </div>

    <div class="container" align="center">
        <img src="superslowmo.jpg" width=25% style="padding: 1em 0em 1em 0em">
        <img src="dogs.jpg" width=34.5% style="padding: 1em 0em 1em 0em">
        <img src="Waterfill-capture_blog-crop-1280x680.jpg" width=25.5% style="padding: 1em 0em 1em 0em">
    </div>

    <!--
    <div class="container w-75">
        <hr>
        <h3>CVPR Virtual Conference Website: <a href="http://cvpr20.com/ai-for-content-creation/">HERE</a></h3>
        <p>
            This contains links to all content and live interactions (video, text chat) for June 15th.
        </p>
    </div>
    -->


    <div class="container w-75">
        <hr>
        <p>
        The AI for Content Creation workshop (AICCW) at CVPR 2020 brings together researchers in computer vision, machine learning, and AI. Content creation has several important applications ranging from virtual reality, videography, gaming, and even retail and advertising. The recent progress of deep learning and machine learning techniques allowed to turn hours of manual, painstaking content creation work into minutes or seconds of automated work. For instance, generative adversarial networks (GANs) have been used to produce photorealistic images of items such as shoes, bags, and other articles of clothing, interior/industrial design, and even computer games' scenes. Neural networks can create impressive and accurate slow-motion sequences from videos captured at standard frame rates, thus side-stepping the need for specialized and expensive hardware. Style transfer algorithms can convincingly render the content of one image with the style of another, offering unique opportunities for generating additional and more diverse training data---in addition to creating awe-inspiring, artistic images.
        Learned priors can also be combined with explicit geometric constraints, allowing for realistic and visually pleasing solutions to traditional problems such as novel view synthesis, in particular for the more complex cases of view extrapolation.
        </p>

        <p>
        AI for content creation lies at the intersection of the graphics, the computer vision, and the design community. However, researchers and professionals in these fields may not be aware of its full potential and inner workings. As such, the workshop is comprised of two parts: techniques for content creation and applications for content creation. The workshop has three goals:
        </p>
        <ol>
            <li>To cover some introductory concepts to help interested researchers from other fields get started in this exciting new area.</li>
            <li>To present selected success cases to advertise how deep learning can be used for content creation.</li>
            <li>Our invited designers will talk about the pain points that designers face using content creation tools.</li>
        </ol>
        <p>
        More broadly, we hope that the workshop will serve as a forum to discuss the latest topics in content creation and the challenges that vision and learning researchers can help solve.
        </p>

        <p>
        Welcome!<br>
        - <a href="https://deqings.github.io/">Deqing Sun</a>, <a href="https://research.nvidia.com/person/mingyu-liu">Ming-Yu Liu</a>, <a href="http://www.lujiang.info/">Lu Jiang</a>, <a href="http://www.jamestompkin.com">James Tompkin</a>, Weilong Yang, and <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>.
        </p>
    </div>

    <div class="container w-75">
        <hr>
        <h3>Morning Session</h3>

        <div id="morningplayer"></div> <!--This will be replaced by an iFrame video player in a script at the bottom of the source.-->

        <p><em>Click â–¶ to jump to each talk!</em></p>

        <table class="table">
        <thead>
            <tr>
                <th>Play</th>
                <th class="w-25">Author</th>
                <th class="w-50">Title</th>
                <th>Links</th>
                <th>Video</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(0)">â–¶</button></td>
                <td><a href="https://deqings.github.io/">Deqing Sun</a></td>
                <td>Introduction</td>
                <td></td>
                <td><a href="https://youtu.be/z8jbOo9EFsI">ðŸ”—</a></td>
            </tr>
            <tr>
                <td colspan="5"><em>Invited Speakers</em></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(185)">â–¶</button></td>
                <td><a href="http://web.mit.edu/phillipi/">Phillip Isola</a></td>
                <td>Generative Models as Data++</td> 
                <td></td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=185">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(1861)">â–¶</button></td>
                <td><a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a></td>
                <td>Generating 3D Content from Images</td>
                <td></td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=1861">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(4122)">â–¶</button></td>
                <td><a href="https://www.cc.gatech.edu/people/irfan-essa">Irfan Essa</a></td>
                <td>AI (CV/ML) for Content Creation</td>
                <td></td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=4122">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(5798)">â–¶</button></td>
                <td><a href="https://cs.stanford.edu/people/jcjohns/">Justin Johnson</a></td>
                <td>Generating 3D Content from 2D Supervision</td>
                <td></td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=5798">ðŸ”—</a></td>
            </tr>
            <tr>
                <td colspan="5"><em>Papers: Session 1</em></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(7723)">â–¶</button></td>
                <td>Tristan Sylvain (Mila); Pengchuan Zhang (Microsoft Research AI); Yoshua Bengio (Mila); R Devon Hjelm, Shikhar Sharma (Microsoft Research)</td>
                <td>Object-centric Image Generation from Layouts<br><em>Extended abstract</em></td>
                <td>[<a href="https://drive.google.com/file/d/1dDEiQikf8IagxgH1Rves5KQ901KrWaCC/view?usp=sharing">PDF</a>]</td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=7723">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(8010)">â–¶</button></td>
                <td>Yuchuan Gou (paii-labs.com); Qiancheng Wu (University of California, Berkeley); Minghao Li, Bo Gong, Mei Han (paii-labs.com)</td>
                <td>SegAttnGAN: Text to Image Generation with Segmentation Attention<br><em>Extended abstract</em></td>
                <td>[<a href="https://arxiv.org/abs/2005.12444">arXiv</a>]</td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=8010">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(8306)">â–¶</button></td>
                <td>Junbum Cha, Sanghyuk Chun, Gayoung Lee, Bado Lee, Seonghyeon Kim, Hwalsuk Lee (Clova AI Research, NAVER Corp.)</td>
                <td>Towards High-quality Few-shot Font Generation with Dual Memory Attention<br><em>Extended abstract</em></td>
                <td>[<a href="https://drive.google.com/file/d/1YQ5QqMVMPRJEGW5i11p0H9m6UIei3ebl/view?usp=sharing">PDF</a>]</td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=8306 ">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(8596)">â–¶</button></td>
                <td>Sangwoo Mo (KAIST); Minsu Cho (POSTECH); Jinwoo Shin (KAIST)</td>
                <td>Freeze the Discriminator: A Simple Baseline for Fine-tuning GANs<br><em>Extended abstract</em></td>
                <td>[<a href="https://arxiv.org/abs/2002.10964">arXiv</a>]</td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=8596">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(8657)">â–¶</button></td>
                <td>Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, Yong Jae Lee (University of California, Davis)</td>
                <td>MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation<br><em>Paper (also at CVPR 2020)</em></td>
                <td>[<a href="https://drive.google.com/open?id=1zCgZpkQcnV6y6m04PGwCl7x3afa_fY6H">PDF</a>] [<a href="https://arxiv.org/abs/1911.11758">arXiv</a>] [<a href="https://github.com/Yuheng-Li/MixNMatch">Project Webpage</a>]</td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=8657">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(8719)">â–¶</button></td>
                <td>Yujun Shen, Bolei Zhou (CUHK)</td>
                <td>Interpreting the Latent Space of GANs for Semantic Face Editing<br><em>Paper (also at CVPR 2020)</em></td>
                <td>[<a href="https://arxiv.org/abs/1907.10786">arXiv</a>] [<a href="https://genforce.github.io/interfacegan/">Project Webpage</a>] [<a href="https://drive.google.com/file/d/1UqHdAuKLDD22K4tLHX1rOoyece_KuHks/view?usp=sharing">Extended Abstract PDF</a>]</td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=8719">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(8842)">â–¶</button></td>
                <td>Rameen Abdal, Peter Wonka (KAUST); Yipeng Qin (Cardiff University)</td>
                <td>Image2StyleGAN++: How to Edit the Embedded Images?<br><em>Paper (also at CVPR 2020)</em></td>
                <td>[<a href="https://arxiv.org/abs/1911.11544">arXiv</a>] [<a href="https://drive.google.com/file/d/1SePD9gAsXCMAlM2dA2na5NjJmo92mWAC/view?usp=sharing">Video (1 min)</a>]</td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=8842">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.seekTo(8906)">â–¶</button></td>
                <td>Peihao Zhu, Rameen Abdal (KAUST); Yipeng Qin (Cardiff University); Peter Wonka (KAUST)</td>
                <td>SEAN: Image Synthesis with Semantic Region-adaptive Normalization<br><em>Paper (also at CVPR 2020)</em></td>
                <td>[<a href="https://arxiv.org/abs/1911.12861v1">arXiv</a>] [<a href="https://zpdesu.github.io/SEAN/">Project Webpage</a>]</td>
                <td><a href="https://youtu.be/z8jbOo9EFsI?t=8906">ðŸ”—</a></td>
            </tr>
        </tbody>
        </table>
    </div>

    <div class="container w-75">
        <hr>
        <h3>Afternoon Session</h3>
        <p>

        <div id="afternoonplayer"></div> <!--This will be replaced by an iFrame video player in a script at the bottom of the source.-->
        <p><em>Click â–¶ to jump to each talk!</em></p>

        <table class="table">
        <thead>
            <tr>
                <th>Play</th>
                <th class="w-25">Author</th>
                <th class="w-50">Title</th>
                <th>Links</th>
                <th>Video</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(121)">â–¶</button></td>
                <td><a href="http://www.jamestompkin.com/">James Tompkin</a></td>
                <td>Introduction</td>
                <td></td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=121">ðŸ”—</a></td>
            </tr>
            <!-- ==================================================== -->
            <tr>
                <td colspan="5"><em>Invited Speakers</em></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(176)">â–¶</button></td>
                <td><a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a></td>
                <td>AI for 3D Content Creation</td> 
                <td></td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=176">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(1936)">â–¶</button></td>
                <td><a href="http://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a></td>
                <td>What if CVPR is a Graphics Conference?</td>
                <td></td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=1936">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(4588)">â–¶</button></td>
                <td><a href="http://ying-cao.com/">Ying Cao</a></td>
                <td>Data-driven Graphic Design: Bringing AI into Graphic Design</td>
                <td></td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=4588">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(6241)">â–¶</button></td>
                <td><a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a></td>
                <td>Learning to Synthesize Image and Video Contents</td>
                <td></td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=6241">ðŸ”—</a></td>
            </tr>
            <!-- ==================================================== -->
            <tr>
                <td colspan="5"><em>Papers: Session 2</em></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(7485)">â–¶</button></td>
                <td>Patrick Esser, Robin Rombach, Bjorn Ommer (Heidelberg University)</td>
                <td>Network Fusion for Content Creation with Conditional INNs<br><em>Paper</em></td>
                <td>[<a href="https://arxiv.org/abs/2005.13580">arXiv</a>] [<a href="https://compvis.github.io/network-fusion/">Webpage</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=7485">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(7792)">â–¶</button></td>
                <td>Tianhao Zhang, Lu Jiang (Google Research); Weilong Yang (Google Inc.)</td>
                <td>Text-guided Image Manipulation via Local Feature Editing<br><em>Paper</em></td>
                <td>[<a href="https://drive.google.com/file/d/1qwy4zm_3bgRPqrm64QTi34eHdRiuDuXx/view?usp=sharing">PDF</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=7792">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(8143)">â–¶</button></td>
                <td>Kuniaki Saito, Kate Saenko (Boston University); Ming-Yu Liu (NVIDIA)</td>
                <td>COCO-FUNIT: Few-shot Unsupervised Image Translation with a Content-conditioned Style Encoder<br><em>Extended abstract</em></td>
                <td>[<a href="https://drive.google.com/file/d/1SNyy40QafGrB_v-pUFprph7RapieK5hE/view?usp=sharing">PDF</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=8143">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(8204)">â–¶</button></td>
                <td>Hung-Yu Tseng, Hsin-Ying Lee (University of California, Merced); Lu Jiang (Google Research); Weilong Yang (Google Inc.); Ming-Hsuan Yang (University of California at Merced)</td>
                <td>RetrieveGAN: Image Synthesis via Differentiable Patch Retrieval<br><em>Extended abstract</em></td>
                <td>[<a href="https://drive.google.com/file/d/1HikWqiKsLb8A5fTKlrB_ubsGU44bEhJb/view?usp=sharing">PDF</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=8204">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(8265)">â–¶</button></td>
                <td>Yunjey Choi, Youngjung Uh (Clova AI Research, NAVER Corp.); Jaejun Yoo (EPFL); Jung-Woo Ha (Clova AI Research, NAVER Corp.)</td>
                <td>StarGAN v2: Diverse Image Synthesis for Multiple Domains<br><em>Paper (also at CVPR2020)</em></td>
                <td>[<a href="https://arxiv.org/abs/1912.01865">arXiv</a>] [<a href="https://github.com/clovaai/stargan-v2/">Project Webpage</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=8265">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(8327)">â–¶</button></td>
                <td>David Stap, Maartje A ter Hoeve, Sarah Ibrahimi (University of Amsterdam)</td>
                <td>Conditional Image Generation and Manipulation for User-specified Content<br><em>Paper</em></td>
                <td>[<a href="https://arxiv.org/abs/2005.04909">arXiv</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=8327">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(8387)">â–¶</button></td>
                <td>Hanxiang Hao, Sriram Baireddy, Amy R. Reibman, Edward Delp (Purdue University)</td>
                <td>FaR-GAN for One-shot Face Reenactment<br><em>Paper</em></td>
                <td>[<a href="https://arxiv.org/abs/2005.06402">arXiv</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=8387">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(8449)">â–¶</button></td>
                <td>Youssef Alami Mejjati (University of Bath); Zejiang Shen, Michael Snower, Aaron Gokaslan (Brown University); Oliver Wang (Adobe Systems Inc); James Tompkin (Brown University); Kwang In Kim (UNIST)</td>
                <td>Generating Object Stamps<br><em>Extended abstract</em></td>
                <td>[<a href="https://drive.google.com/file/d/1hx0mulP1Zxv8nSF5_lfdU5fNSWeQH8fU/view?usp=sharing">PDF</a>] [<a href="https://arxiv.org/abs/2001.02595">arXiv</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=8449">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(8521)">â–¶</button></td>
                <td>Qingyuan Zheng (University of Maryland, Baltimore County); Zhuoru Li (Project HAT); Adam Bargteil (University of Maryland, Baltimore County)</td>
                <td>Learning to Shadow Hand-drawn Sketches<br><em>Paper (also at CVPR2020)</em></td>
                <td>[<a href="https://arxiv.org/abs/2002.11812">arXiv</a>] [<a href="https://cal.cs.umbc.edu/Papers/Zheng-2020-Shade/">Project Webpage</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=8521">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(8819)">â–¶</button></td>
                <td>Shan-Jean Wu, Chih-Yuan Yang, Jane Yung-jen Hsu (National Taiwan University)</td>
                <td>CalliGAN: Style and Structure-aware Chinese Calligraphy Generator<br><em>Paper</em></td>
                <td>[<a href="http://arxiv.org/abs/2005.12500">arXiv</a>] [<a href="https://github.com/JeanWU/CalliGAN">Code</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=8819">ðŸ”—</a></td>
            </tr>
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(8880)">â–¶</button></td>
                <td>Surgan Jandial* (IIT Hyderabad); Ayush Chopra* (Media and Data Science Research Lab, Adobe); Balaji Krishnamurthy* (Media and Data Science Research Lab, Adobe); Kumar Ayush (Stanford University); Mayur Hemani (Media and Data Science Research Lab, Adobe); Abhijeet Halwai (Microsoft Research)</td>
                <td>SieveNet: A Unified Framework for Image Based Virtual Try-On<br><em>Paper (also at WACV2020)</em></td>
                <td>[<a href="https://arxiv.org/abs/2001.06265">arXiv</a>]</td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=8880">ðŸ”—</a></td>
            </tr>
            <!-- ==================================================== -->
            <tr>
                <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.seekTo(9082)">â–¶</button></td>
                <td colspan="3"><em>Closing remarks</em></td>
                <td><a href="https://youtu.be/DrizewlvZGc?t=9082">ðŸ”—</a></td>
            </tr>
        </tbody>
        </table>
       

    </div>

    <!-- <div class="container w-75">
        <hr>
        <h3>Schedule / Speakers (PDT times; second time repeat recording)</h3>
        <p>
            All sessions are <a href="http://cvpr20.com/ai-for-content-creation/">here</a> (CVPR2020 login required).
        </p>
        <ul>
            <li>09:00 / 21:00 - <a href="http://web.mit.edu/phillipi/">Phillip Isola</a></li>
            <li>09:25 / 21:25 - <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a></li>
            <li>10:10 / 22:10 - <a href="https://www.cc.gatech.edu/people/irfan-essa">Irfan Essa</a></li>
            <li>10:35 / 22:35 - <a href="https://cs.stanford.edu/people/jcjohns/">Justin Johnson</a></li>
            <li>11:00 / 23:00 - Papers video session 1 / breakout Zoom sessions</li>
            <li><em>Break</em></li>
            <li>14:00 / 02:00 June 16 - <a href="http://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a></li>
            <li>14:25 / 02:25 June 16 - <a href="http://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a></li>
            <li>15:15 / 03:15 June 16 - <a href="http://ying-cao.com/">Ying Cao</a></li>
            <li>15:35 / 03:35 June 16 - <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a></li>
            <li>16:00 / 04:00 June 16 - Papers video session 2 / breakout Zoom sessions</li>
        </ul>
    </div> -->

    <div class="container w-75">
        <hr>
        <h3>Awards</h3>
        <h4>Best paper</h4>
        <ul>
            <li>Network Fusion for Content Creation with Conditional INNs<br><em>Patrick Esser (Heidelberg University); Robin Rombach (Heidelberg University); Bjorn Ommer (Heidelberg University)</em><br>[<a href="https://arxiv.org/abs/2005.13580">arXiv</a>] [<a href="https://compvis.github.io/network-fusion/">Webpage</a>]</li>
        </ul>

        <h4>Best paper runner up</h4>
        <ul>
            <li>Toward High-quality Few-shot Font Generation with Dual Memory<br><em>Junbum Cha (Clova AI Research, NAVER Corp.); Sanghyuk Chun (Clova AI Research, NAVER Corp.); Gayoung Lee (Clova AI Research, NAVER Corp.); Bado Lee (Clova AI Research, NAVER Corp.); Seonghyeon Kim (Clova AI Research, NAVER Corp.); Hwalsuk Lee (Clova AI Research, NAVER Corp.)</em><br>[<a href="https://drive.google.com/file/d/1YQ5QqMVMPRJEGW5i11p0H9m6UIei3ebl/view?usp=sharing">PDF</a>]</li>
        </ul>

        <h4>Audience choice</h4>
        <ul>
            <li>SegAttnGAN: Text to Image Generation with Segmentation Attention<br><em>Yuchuan Gou (paii-labs.com); Qiancheng Wu (University of California, Berkeley); Minghao Li (paii-labs.com); Bo Gong (paii-labs.com); Mei Han (paii-labs.com)</em><br>[<a href="https://arxiv.org/abs/2005.12444">arXiv</a>]</li>            
        </ul>
    </div>

    <div class="container w-75">
        <hr>
        <h3>All accepted works (in random order)</h3>
        <h4>Extended abstracts (4 pages)</h4>
        <ul>
            <li>Mimicry: Towards the Reproducibility of GAN Research<br><em>Kwot Sin Lee (University of Cambridge); Christopher Town (University of Cambridge)</em><br>[<a href="https://arxiv.org/abs/2005.02494">arXiv</a>] [<a href="https://github.com/kwotsin/mimicry">Code</a>] [<a href="https://drive.google.com/file/d/1jMTHrY1jCaRQj2V1VwvE_pAhb43VeOJa/view?usp=sharing">Video (1 min)</a>]</li>
            <li>Object-Centric Image Generation from Layouts<br><em>Tristan Sylvain (Mila); Pengchuan Zhang (Microsoft Research AI); Yoshua Bengio (Mila); R Devon Hjelm (Microsoft Research); Shikhar Sharma (Microsoft Research)</em><br>[<a href="https://drive.google.com/file/d/1dDEiQikf8IagxgH1Rves5KQ901KrWaCC/view?usp=sharing">PDF</a>]</li>
            <li>Freeze the Discriminator: a Simple Baseline for Fine-Tuning GANs<br><em>Sangwoo Mo (KAIST); Minsu Cho (POSTECH); Jinwoo Shin (KAIST)</em><br>[<a href="https://arxiv.org/abs/2002.10964">arXiv</a>]</li>
            <li>SegAttnGAN: Text to Image Generation with Segmentation Attention<br><em>Yuchuan Gou (paii-labs.com); Qiancheng Wu (University of California, Berkeley); Minghao Li (paii-labs.com); Bo Gong (paii-labs.com); Mei Han (paii-labs.com)</em><br>[<a href="https://arxiv.org/abs/2005.12444">arXiv</a>]</li>
            <li>Toward High-quality Few-shot Font Generation with Dual Memory<br><em>Junbum Cha (Clova AI Research, NAVER Corp.); Sanghyuk Chun (Clova AI Research, NAVER Corp.); Gayoung Lee (Clova AI Research, NAVER Corp.); Bado Lee (Clova AI Research, NAVER Corp.); Seonghyeon Kim (Clova AI Research, NAVER Corp.); Hwalsuk Lee (Clova AI Research, NAVER Corp.)</em><br>[<a href="https://drive.google.com/file/d/1YQ5QqMVMPRJEGW5i11p0H9m6UIei3ebl/view?usp=sharing">PDF</a>]</li>
            <li>Generating Object Stamps<br><em>Youssef Alami Mejjati (University of Bath); Zejiang Shen (Brown University); Michael Snower (Brown University); Aaron Gokaslan (Brown University); Oliver Wang (Adobe Systems Inc); James Tompkin (Brown University); Kwang In Kim (UNIST)</em><br>[<a href="https://drive.google.com/file/d/1hx0mulP1Zxv8nSF5_lfdU5fNSWeQH8fU/view?usp=sharing">PDF</a>]</li>
            <li>COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder<br><em>Kuniaki Saito (Boston University); Kate Saenko (Boston University); Ming-Yu Liu (NVIDIA)</em><br>[<a href="https://drive.google.com/file/d/1SNyy40QafGrB_v-pUFprph7RapieK5hE/view?usp=sharing">PDF</a>]</li>
            <li>RetrieveGAN: Image Synthesis via Differentiable Patch Retrieval<br><em>Hung-Yu Tseng (University of California, Merced); Hsin-Ying Lee (University of California, Merced); Lu Jiang (Google Research); Weilong Yang (Google Inc.); Ming-Hsuan Yang (University of California at Merced)</em><br>[<a href="https://drive.google.com/file/d/1HikWqiKsLb8A5fTKlrB_ubsGU44bEhJb/view?usp=sharing">PDF</a>]</li>
            <li>Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis<br><em>Yujun Shen (Dept. of IE, CUHK); Ceyuan Yang (Chinese University of Hong Kong); Bolei Zhou (CUHK)</em><br>[<a href="https://drive.google.com/file/d/1FnhMOcVzlx6ne1E2WwPZVxREkVY_NTVj/view?usp=sharing">PDF</a>] [<a href="https://arxiv.org/abs/1911.09267">arXiv</a>] [<a href="https://genforce.github.io/higan/">Project Webpage</a>]</li>
        </ul>

        <h4>Papers (8 pages)</h4>
        <ul>
            <li>Conditional Image Generation and Manipulation for User-Specified Content<br><em>David Stap (University of Amsterdam); Maartje A ter Hoeve (University of Amsterdam); Sarah Ibrahimi (University of Amsterdam)</em><br>[<a href="https://arxiv.org/abs/2005.04909">arXiv</a>]</li>
            <li>FaR-GAN for One-Shot Face Reenactment<br><em>Hanxiang Hao (Purdue University); Sriram Baireddy (Purdue University); Amy R. Reibman (Purdue University); Edward Delp (Purdue University)</em><br>[<a href="https://arxiv.org/abs/2005.06402">arXiv</a>]</li>
            <li>CalliGAN: Style and Structure-aware Chinese Calligraphy Character Generator<br><em>Shan-Jean Wu (National Taiwan University); Chih-Yuan Yang (National Taiwan University); Jane Yung-jen Hsu (National Taiwan University)</em><br>[<a href="http://arxiv.org/abs/2005.12500">arXiv</a>] [<a href="https://github.com/JeanWU/CalliGAN">Code</a>]</li>
            <li>Network Fusion for Content Creation with Conditional INNs<br><em>Patrick Esser (Heidelberg University); Robin Rombach (Heidelberg University); Bjorn Ommer (Heidelberg University)</em><br>[<a href="https://arxiv.org/abs/2005.13580">arXiv</a>] [<a href="https://compvis.github.io/network-fusion/">Webpage</a>]</li>
            <li>Text-guided Image Manipulation via Local Feature Editing<br><em>Tianhao Zhang (Google Research); Lu Jiang (Google Research); Weilong Yang (Google Inc.)</em><br>[<a href="https://drive.google.com/file/d/1qwy4zm_3bgRPqrm64QTi34eHdRiuDuXx/view?usp=sharing">PDF</a>]</li>
        </ul>
        
        <h4>Papers (8 pages)&mdash;also in other proceedings</h4>
        <ul>
            <li>Learning to Shadow Hand-drawn Sketches<br><em>Qingyuan Zheng (University of Maryland, Baltimore County); Zhuoru Li (Project HAT); Adam Bargteil (University of Maryland, Baltimore County)</em><br>CVPR 2020<br>[<a href="https://arxiv.org/abs/2002.11812">arXiv</a>] [<a href="https://cal.cs.umbc.edu/Papers/Zheng-2020-Shade/">Project Webpage</a>]</li>
            <li>SEAN: Image Synthesis with Semantic Region-Adaptive Normalization<br><em>Peihao Zhu (KAUST); Rameen Abdal (KAUST); Yipeng Qin (Cardiff University); Peter Wonka (KAUST)</em><br>CVPR 2020<br>[<a href="https://arxiv.org/abs/1911.12861v1">arXiv</a>] [<a href="https://zpdesu.github.io/SEAN/">Project Webpage</a>]</li>
            <li>MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation<br><em>Yuheng Li (University of California Davis); Krishna Kumar Singh (University of California Davis); Utkarsh Ojha (University of California, Davis); Yong Jae Lee (University of California, Davis)</em><br>CVPR 2020<br>[<a href="https://drive.google.com/open?id=1zCgZpkQcnV6y6m04PGwCl7x3afa_fY6H">PDF</a>] [<a href="https://arxiv.org/abs/1911.11758">arXiv</a>] [<a href="https://github.com/Yuheng-Li/MixNMatch">Project Webpage</a>]</li>
            <li>Interpreting the Latent Space of GANs for Semantic Face Editing<br><em>Yujun Shen (Dept. of IE, CUHK); Bolei Zhou (CUHK)</em><br>CVPR 2020<br>[<a href="https://arxiv.org/abs/1907.10786">arXiv</a>] [<a href="https://genforce.github.io/interfacegan/">Project Webpage</a>] [<a href="https://drive.google.com/file/d/1UqHdAuKLDD22K4tLHX1rOoyece_KuHks/view?usp=sharing">Extended Abstract PDF</a>]</li>
            <li>StarGAN v2: Diverse Image Synthesis for Multiple Domains<br><em>Yunjey Choi (Clova AI Research, NAVER Corp.); Youngjung Uh (Clova AI Research, NAVER Corp.); Jaejun Yoo (EPFL); Jung-Woo Ha (Clova AI Research, NAVER Corp.)</em><br>CVPR 2020<br>[<a href="https://arxiv.org/abs/1912.01865">arXiv</a>] [<a href="https://github.com/clovaai/stargan-v2/">Project Webpage</a>]</li>
            <li>SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On<br><em>Surgan Jandial* (IIT Hyderabad); Ayush Chopra* (Media and Data Science Research Lab, Adobe); Balaji Krishnamurthy* (Media and Data Science Research Lab, Adobe); Kumar Ayush (Stanford University); Mayur Hemani (Media and Data Science Research Lab, Adobe); Abhijeet Halwai (Microsoft Research)</em><br>WACV 2020<br>[<a href="https://arxiv.org/abs/2001.06265">arXiv</a>]</li>
            <li>Image2StyleGAN++: How to Edit the Embedded Images?<br><em>Rameen Abdal (KAUST); Peter Wonka (KAUST); Yipeng Qin (Cardiff University)</em><br>CVPR 2020<br>[<a href="https://arxiv.org/abs/1911.11544">arXiv</a>] [<a href="https://drive.google.com/file/d/1SePD9gAsXCMAlM2dA2na5NjJmo92mWAC/view?usp=sharing">Video (1 min)</a>]</li>
        </ul>
    </div>

    <!--
    <div class="container w-75">
        <hr>
        <h3>Submission Instructions</h3>
        <p>
        We call for papers (8 pages not including references) and extended abstracts (4 pages not including references) to be showcased in a poster session, and for interactive demos, both for the AI for Content Creation Workshop at CVPR 2020. Authors of accepted papers and extended abstracts will be asked to post their submissions on arXiv. Both papers and extended abstracts are not archival and will not be included in the proceedings of CVPR 2020 (authors should be aware that some conferences consider peer-reviewed works with >4-pages to be in violation of double submission policies, e.g., <a href="https://eccv2020.eu/author-instructions/">ECCV</a>). We will accept work in progress, work that has not been published elsewhere, and work that has been recently published elsewhere including at CVPR 2020. In the interests of fostering a free exchange of ideas, we welcome both novel and previously-published work.
        </p>
        
        <p>
            Paper submissions are <em>double blind</em> and in the <a href="http://cvpr2020.thecvf.com/submission/main-conference/author-guidelines#submission-guidelines">CVPR template</a>.
        </p>

        <p>
        Paper submission: <strike>March 20, 2020</strike> March 31, 2020, 11:59 PST<br>
        Acceptance notification: April 18, 2020<br>
        Submission Website: <a href="https://cmt3.research.microsoft.com/AI4CCW2020">https://cmt3.research.microsoft.com/AI4CCW2020</a>
        </p>

        <p>
        The best paper and the best demo will be acknowledged with a Titan RTX GPU (kindly provided by our sponsors).
        </p>

        <p>
        We seek contributions on a variety of aspects on content creation, including but not limited to the following areas:
        </p>
        <ul>
            <li>Generative models for image/video synthesis</li>
            <li>Image/video editing</li>
            <li>Image/video inpainting</li>
            <li>Image/video extrapolation</li>
            <li>Image/video translation</li>
            <li>Style transfer</li>
            <li>Text-to-image creation</li>
            <li>...</li>
        </ul>
            
        <p>
        This includes domains and applications for content creation:
        </p>    
            
        <ul>
            <li>Image and video for enthusiast, VFX, architecture, advertisements, art, ...</li>
            <li>2D/3D graphic design</li>
            <li>Text and typefaces</li>
            <li>Design for documents, Web</li>
            <li>Fashion, garments, and outfits</li>
            <li>Novel applications and datasets</li>
            <li>...</li>
        </ul>    
    </div>
    -->

    <div class="container w-75">
        <hr>
        <h4>Precursors</h4>
        <ul>
            <li>2019 - <a href="https://nvlabs.github.io/dl-for-content-creation/">Deep Learning for Content Creation</a> (Tutorial at CVPR)</li>
        </ul>
    </div>

    
    <div class="container" align="center">
        <img src="google_logo.png" width=20% style="padding: 1em 1em 1em 1em">
        <img src="brown-cs-logo.png" width=22% style="padding: 1em 1em 1em 1em">
        <img src="nvidia_logo.png" width=12% style="padding: 1em 1em 1em 1em">
        <img src="Adobe-logo.jpg" width=22% style="padding: 1em 1em 1em 1em">
    </div>

    <!-- Javascript to control YouTube player-->
    <script>
        // From https://developers.google.com/youtube/iframe_api_reference
        document.addEventListener("DOMContentLoaded", function(event) {
            var tag = document.createElement('script');
            tag.src = "https://www.youtube.com/iframe_api";
            var firstScriptTag = document.getElementsByTagName('script')[0];
            firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
        });

        var morningplayer, afternoonplayer;
        function onYouTubeIframeAPIReady() {
            morningplayer = new YT.Player('morningplayer', {width: '800', height: '450', videoId: 'z8jbOo9EFsI'});
            afternoonplayer = new YT.Player('afternoonplayer', {width: '800', height: '450', videoId: 'DrizewlvZGc'});
        }
    </script>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

  </body>
</html>