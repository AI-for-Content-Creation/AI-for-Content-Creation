<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <link href="css/theme_1610153848925.css" rel="stylesheet"> <!-- Via Themestr.app -->

    <!-- Option 1: Bootstrap Bundle with Popper -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>

    <style>
    p {
        text-align: justify;
        -webkit-hyphens: auto;
            -moz-hyphens: auto;
        hyphens: auto;
    }

    /* 
    .brownbrown {
        color: #4E3629;
    }

    .brownred {
        color: #C00404;
    } */

    .logo {
        width: 20em;
        padding: 1em 1em 1em 1em;
    }
    </style>


  <title>AI4CC 2022</title>
  </head>

  <body>
    
    <header class="bg-light text-dark py-5">
    <div class="container text-center">
        <h1>AI for Content Creation Workshop</h1>
        <h3>@ CVPR 2022</h3>
        <h4>19th June 2022</h4>
    </div>
    </header>

    <!-- A grey horizontal navbar that becomes vertical on small screens -->
    <nav class="navbar sticky-top navbar-expand-sm" style="margin-top:2em; background: #FFF;">
        <div class="container">
            <a class="navbar-brand" href="#">AI4CC 2022</a>
            
            <ul class="navbar-nav">
            <li class="nav-item">
                <a class="nav-link link-primary" href="#summary">Summary</a>
            </li>
            <!--
            <li class="nav-item">
                <a class="nav-link link-primary" href="#awards">Awards</a>
            </li>
            -->
            <li class="nav-item">
                <a class="nav-link link-primary" href="#schedule">Schedule</a>
            </li>
            <!--
            <li class="nav-item">
                <a class="nav-link link-primary" href="#papers">Papers</a>
            </li>
            <li class="nav-item">
                <a class="nav-link link-primary" href="#competitions">Competitions</a>
            </li>
            <li class="nav-item">
                <a class="nav-link link-primary" href="#submission">Submission</a>
            </li>
            -->
            <li class="nav-item">
                <a class="nav-link link-primary" href="#previousworkshops">Previous Workshops</a>
            </li>    
            </ul>
        </div>
    </nav>


    <div class="container" style="margin-top:2em">
        <div id="carouselIndicators" class="carousel slide" data-ride="carousel">
            <ol class="carousel-indicators">
                <li data-target="#carouselIndicators" data-slide-to="0" class="active"></li>
                <li data-target="#carouselIndicators" data-slide-to="1"></li>
                <li data-target="#carouselIndicators" data-slide-to="2"></li>
                <li data-target="#carouselIndicators" data-slide-to="3"></li>
            </ol>

            <div class="carousel-inner">
                <div class="carousel-item text-center active">
                    <img height="300px" src="images/rombach_aicc2020_network-fusion_exemplarguided_crop.jpg" alt="Network Fusion for Content Creation with Conditional INNs">
                    <div class="carousel-caption d-none d-md-block">
                    <h5 class="text-dark text-center" style="background: #FFF;">Rombach, Esser, and Ommer, AICC 2020</h5>
                    <p class="text-dark text-center" style="background: #FFF;">Network Fusion for Content Creation with Conditional INNs</p>
                    </div>
                </div>

                <div class="carousel-item text-center" style="height: 300px">
                    <img width="100%" src="images/Cha_aicc2020_few-shot-font-generation.jpg" alt="Toward High-quality Few-shot Font Generation with Dual Memory">
                    <div class="carousel-caption d-none d-md-block">
                    <h5 class="text-dark text-center" style="background: #FFF;">Cha et al., AICC 2020</h5>
                    <p class="text-dark text-center" style="background: #FFF;">Toward High-quality Few-shot Font Generation with Dual Memory</p>
                    </div>
                </div>

                <div class="carousel-item text-center">
                    <img height="300px" src="images/sylvain_aicc2020_object-centric_image_generation.jpg" alt="Object-Centric Image Generation from Layouts">
                    <div class="carousel-caption d-none d-md-block">
                    <h5 class="text-dark text-center" style="background: #FFF;">Sylvain et al., AICC 2020</h5>
                    <p class="text-dark text-center" style="background: #FFF;">Object-Centric Image Generation from Layouts</p>
                    </div>
                </div>

                <div class="carousel-item text-center">
                    <img height="300px" src="images/jahn_aicc2021_high-res-complex-transformers.jpg" alt="High-Resolution Complex Scene Synthesis with Transformers">
                    <div class="carousel-caption d-none d-md-block">
                    <h5 class="text-dark text-center" style="background: #FFF;">Jahn et al., AICC 2021</h5>
                    <p class="text-dark text-center" style="background: #FFF;">High-Resolution Complex Scene Synthesis with Transformers</p>
                    </div>
                </div>
            </div>

            <a class="carousel-control-prev" href="#carouselExampleIndicators" role="button" data-slide="prev">
                <span class="carousel-control-prev-icon" aria-hidden="true"></span>
                <span class="sr-only">Previous</span>
            </a>
            <a class="carousel-control-next" href="#carouselExampleIndicators" role="button" data-slide="next">
                <span class="carousel-control-next-icon" aria-hidden="true"></span>
                <span class="sr-only">Next</span>
            </a>
        </div>
        <br><br>
    </div>


    <div class="container" id="summary">
        <h3>Summary</h3>
        <p>
        The AI for Content Creation (AI4CC) workshop at CVPR 2022 brings together researchers in computer vision, machine learning, and AI. Content creation is required for simulation and training data generation, media like photography and videography, virtual reality and gaming, art and design, and documents and advertising (to name just a few application domains).
        Recent progress in machine learning, deep learning, and AI techniques has allowed us to turn hours of manual, painstaking content creation work into minutes or seconds of automated or interactive work.
        For instance, generative adversarial networks (GANs) can produce photorealistic images of 2D and 3D items such as humans, landscapes, interior scenes, virtual environments, or even industrial designs.
        Neural networks can super-resolve and super-slomo videos, interpolate between photos with intermediate novel views and even extrapolate, and transfer styles to convincingly render and reinterpret content.
        In addition to creating awe-inspiring artistic images, these offer unique opportunities for generating additional and more diverse training data.
        Learned priors can also be combined with explicit appearance and geometric constraints, perceptual understanding, or even functional and semantic constraints of objects.
        </p>

        <p>
        AI for content creation lies at the intersection of the graphics, the computer vision, and the design community. However, researchers and professionals in these fields may not be aware of its full potential and inner workings. As such, the workshop is comprised of two parts: techniques for content creation and applications for content creation. The workshop has three goals:
        </p>
        <ol>
            <li>To cover introductory concepts to help interested researchers from other fields start in this exciting area.</li>
            <li>To present success stories to show how deep learning can be used for content creation.</li>
            <li>To discuss pain points that designers face using content creation tools.</li>
        </ol>
        <p>
        More broadly, we hope that the workshop will serve as a forum to discuss the latest topics in content creation and the challenges that vision and learning researchers can help solve.
        </p>

        <p>
        Welcome! - <br>
        <a class="link-primary" href="https://deqings.github.io/">Deqing Sun (Google)</a> <a href="https://twitter.com/DeqingSun"><img src="images/logos/Twitter_Logo.png" width=18px></a> <br>
        <a class="link-primary" href="https://www.linkedin.com/in/huiwen-chang-999962156/">Huiwen Chang (Google)</a> <br>
        <a class="link-primary" href="https://www.weizmann.ac.il/math/dekel/">Tali Dekel (Weizmann Institute)</a> <a href="https://twitter.com/talidekel/"><img src="images/logos/Twitter_Logo.png" width=18px></a> <br>
        <a class="link-primary" href="http://www.lujiang.info/">Lu Jiang (Google)</a> <a href="https://twitter.com/roadjiang"><img src="images/logos/Twitter_Logo.png" width=18px></a> <br>
        <a class="link-primary" href="https://liaojing.github.io/html/">Liao Jing (City University of Hong Kong)</a> <br>
        <a class="link-primary" href="https://research.nvidia.com/person/mingyu-liu">Ming-Yu Liu (NVIDIA)</a> <a href="https://twitter.com/liu_mingyu"><img src="images/logos/Twitter_Logo.png" width=18px></a> <br>
        <a class="link-primary" href="https://research.adobe.com/person/jingwan-lu">Cynthia Lu (Adobe)</a> <a href="https://twitter.com/JingwanL"><img src="images/logos/Twitter_Logo.png" width=18px></a> <br>
        <a class="link-primary" href="https://seungjunnah.github.io/">Seungjun Nah (NVIDIA)</a> <br>
        <a class="link-primary" href="http://www.jamestompkin.com/">James Tompkin (Brown)</a> <a href="https://twitter.com/jtompkin"><img src="images/logos/Twitter_Logo.png" width=18px></a> <br>
        <a class="link-primary" href="https://tcwang0509.github.io/">Ting-Chun Wang (NVIDIA)</a> <a href="https://twitter.com/tcwang0509/"><img src="images/logos/Twitter_Logo.png" width=18px></a> <br>
        <a class="link-primary" href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu (Carnegie Mellon)</a> <a href="https://twitter.com/junyanz89/"><img src="images/logos/Twitter_Logo.png" width=18px></a> <br>
        
        <!--
        Steering committee
        
        <a class="link-primary" href="https://www.cs.utoronto.ca/~fidler">Sanja Fidler (UToronto / NVIDIA)</a>, <a href="https://twitter.com/FidlerSanja"><img src="images/logos/Twitter_Logo.png" width=18px> <br>    
        <a class="link-primary" href="https://people.eecs.berkeley.edu/~kanazawa">Angjoo Kanazawa (UC Berkeley)</a>, <a href="https://twitter.com/akanazawa"><img src="images/logos/Twitter_Logo.png" width=18px> <br>
        <a class="link-primary" href="http://www.kalyans.org/">Kalyan Sunkavalli (Adobe)</a> <br>
        <a class="link-primary" href="https://research.google/people/107064/">Weilong Yang (Waymo)</a>. <br> 
        -->
            
        </p>
    </div>

    <div class="container text-center">
        <img src="images/superslowmo.jpg" width=25% style="padding: 1em 0em 1em 0em">
        <img src="images/dogs.jpg" width=34.5% style="padding: 1em 0em 1em 0em">
        <img src="images/Waterfill-capture_blog-crop-1280x680.jpg" width=25.5% style="padding: 1em 0em 1em 0em">
        <br><br>
    </div>

    <!--
    <div class="container w-75">
        <hr>
        <h3 id="awards">Awards</h3>
        <h4>Best paper</h4>
        <ul>
            <li>
            </li>
        </ul>
    </div>
    -->

    <div class="container" id="schedule">
        <h3>Schedule</h3>
        <!--
        <h3 id="videorecording">Video Recording <a href="https://www.youtube.com/watch?v=x6cHOulDXUo&list=PLNPyQ_mnkEr7ae_dbhv4MG2ja5EI74lzu">(YouTube Playlist)</a></h3>
        -->
        
        Morning session:<br>
        <!--
            <div id="morningplayer"></div>
            <p><em>Click â–¶ to jump to each talk!</em></p>
        -->

        <table class="table table-sm table-striped">
            <thead>
                <tr>
                    <th scope="col"><!--Play--></th><th scope="col">Time CDT</th><th scope="col"></td><th scope="col"></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td></td>
                    <td>08:50</td>
                    <td>Welcome and introductions</td>
                    <td>ðŸ‘‹</td>
                </tr>
                <tr>
                    <td>
                        <!-- STORE FOR LATER
                            <button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.loadVideoById('x6cHOulDXUo')">â–¶</button>
                        -->
                    </td>
                    <td>09:00</td>
                    <td><a href="https://ps.is.mpg.de/~black">Michael Black (MPI for Intelligent Systems)</a></td>
                    <td>
                        <!-- Twitter handle -->
                        <a href="https://twitter.com/Michael_J_Black"><img src="images/logos/Twitter_Logo.png" width=18px></a>
                    </td>
                </tr>
                <tr>
                    <td></td>
                    <td>09:25</td>
                    <td><a href="https://www.ohadf.com/">Ohad Fried (TU Munich)</a></td>
                    <td></td>
                </tr>
                <tr>
                    <td></td>
                    <td>09:50</td>
                    <td><a href="https://ommer-lab.com/">BjÃ¶rn Ommer (University of Munich)</a></td>
                    <td></td>
                </tr>
                <tr>
                    <td></td>
                    <td>10:15</td>
                    <td>Coffee break</td>
                    <td>â˜•</td>
                </tr>
                <tr>
                    <td></td>
                    <td>10:30</td>
                    <td><a href="http://elisaricci.eu/">Elisa Ricci (University of Trento)</a></td>
                    <td><a href="https://twitter.com/eliricci_"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td></td>
                    <td>10:55</td>
                    <td><a href="https://www.duygu-ceylan.com/">Duygu Ceylan (Adobe)</a></td>
                    <td></td>
                </tr>
                <tr>
                    <td></td>
                    <td>11:20</td>
                    <td><a href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky (Princeton)</a></td>
                    <td><a href="https://twitter.com/orussakovsky"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td></td>
                    <td>11:45</td>
                    <td>Poster session 1
                        <ul>
                            <li>Sun et al., <a href="https://arxiv.org/abs/2205.03743">End-to-End Rubbing Restoration Using Generative Adversarial Networks</a></li>
                            <li>Lee et al., <a href="https://arxiv.org/abs/2107.11041v2">RewriteNet: Reliable Scene Text Editing with Implicit Decomposition of Text Contents and Styles</a> | <a href="https://github.com/clovaai/rewritenet">Web+Code</a></li>
                            <li>Sun et al., <a href="https://arxiv.org/abs/2111.09298">SeCGAN: Parallel Conditional Generative Adversarial Networks for Face Editing via Semantic Consistency</a></li>
                            <li>Turgutlu et al., <a href="https://arxiv.org/abs/2205.00347">LayoutBERT: Masked Language Layout Model for Object Insertion</a></li>
                            <li>Lee et al., <a href="https://arxiv.org/abs/2205.06611">StyLandGAN: A StyleGAN based Landscape Image Synthesis using Depth-map</a></li>
                            <li>Poirier-Ginter et al., <a href="https://arxiv.org/abs/2205.06304">Overparameterization Improves StyleGAN Inversion</a> | <a href="https://lvsn.github.io/OverparamStyleGAN/">Webpage</a></li>
                            <li>MenÃ©ndez GonzÃ¡lez et al., <a href="https://arxiv.org/abs/2205.07014">SaiNet: Inpainting behind objects using geometrically meaningful masks.</a></li>
                            <li>Jenni et al., <a href="https://arxiv.org/abs/2205.05609">Video-ReTime: Learning Temporally Varying Speediness for Time Remapping</a></li>
                            <!--<li>Authors et al., <a href="https://arxiv.org/abs/link_here">Improved Image Generation via Sparsity</a> | <a href="additionallinks">SupplDoc</a></li>
                            <li>Authors et al., <a href="https://arxiv.org/abs/link_here">On Conditioning the Input Noise for Controlled Image Generation with Diffusion Models</a> | <a href="additionallinks">SupplDoc</a></li>
                            <li>Authors et al., <a href="https://arxiv.org/abs/link_here">Towards Unified Keyframe Propagation Models</a> | <a href="additionallinks">SupplDoc</a></li>-->
                        </ul>
                    </td>    
                    <td></td>
                </tr>
                <tr>
                    <td></td>
                    <td>12:45</td>
                    <td>Lunch break</td>
                    <td>ðŸ¥ª</td>
                </tr>
            </tbody>
        </table>


        <br><br>
        Afternoon session:<br>
        <!--
        <div id="afternoonplayer"></div>
        <p><em>Click â–¶ to jump to each talk!</em></p>
        -->
        
        <table class="table table-sm table-striped">
            <thead>
                <tr>
                    <th scope="col"><!--Play--></th><th scope="col">Time CDT</th><th scope="col"></td><th scope="col"></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td></td>
                    <td>13:30</td>
                    <td>Oral session</td>
                    <td></td>
                </tr>
                <tr>
                    <td><!-- STORE FOR LATER 
                        <button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.loadVideoById('DJzMLa8zuJQ')">â–¶</button>
                        -->
                    </td>
                    <td>14:00</td>
                    <td><a href="http://adityaramesh.com/">Aditya Ramesh (Open AI)</a></td>
                    <td><a href="https://twitter.com/model_mechanic"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td></td>
                    <td>14:25</td>
                    <td>Coffee break</td>
                    <td>â˜•</td>
                </tr>
                <tr>
                    <td></td>
                    <td>14:45</td>
                    <td><a href="https://baulab.info/">David Bau (Northeastern University)</a></td>
                    <td><a href="https://twitter.com/davidbau"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                
                <tr>
                    <td></td>
                    <td>15:10</td>
                    <td><a href="https://homes.cs.washington.edu/~adriana/">Adriana Schulz (University of Washington)</a></td>
                    <td><a href="https://twitter.com/AdrianaSchulz7"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td></td>
                    <td>15:35</td>
                    <td><a href="https://xunhuang.me/">Xun Huang (NVIDIA)</a></td>
                    <td><a href="https://twitter.com/xunhuang1995"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td></td>
                    <td>16:00</td>
                    <td><a href="http://richzhang.github.io/">Richard Zhang (Adobe)</a></td>
                    <td><a href="https://twitter.com/rzhang88"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td></td>
                    <td>16:25</td>
                    <td>Poster session 2
                        <ul>
                            <li>Authors et al., <a href="https://arxiv.org/abs/link_here">TITLE</a> | <a href="additionallinks">SupplDoc</a></li>
                            <li>Authors et al., <a href="https://arxiv.org/abs/link_here">TITLE</a> | <a href="additionallinks">SupplDoc</a></li>
                        </ul>
                    </td>
                    <td></td>
                </tr>
            </tbody>
        </table>
    </div>


    <div class="container ">
        <hr>
        <h3 id="papers">Presented Works Accepted at Other Venues</h3>
        <p>In random order</p>

        <!-- EXAMPLE FROM 2021
        <ul>
            <li>Few-shot Font Generation with Localized Style Representations and Factorization<br>
                <em>Song Park (Yonsei University); Sanghyuk Chun (NAVER AI LAB); Junbum Cha (Clova AI Research, NAVER Corp.); Bado Lee (Clova AI Research, NAVER Corp.); Hyunjung Shim (Yonsei University)</em><br>
                AAAI 2021<br>
                [<a href="https://arxiv.org/abs/2009.11042">arXiv</a>] [<a href="https://github.com/clovaai/lffont">Webpage</a>]
            </li>
        </ul>
        -->
    </div>


    <!--
    <div class="container" id="speakers">
        <hr>
        <h3>Invited Speakers</h3>

        <ul>
            <li>Michael Black (Max Planck Institute for Intelligent Systems)</li>
            <li>Olga Russakovsky (Princeton)</li>
            <li>Duygu Ceylan (Adobe)</li>
            <li>Adriana Schulz (University of Washington)</li>
            <li>Richard Zhang (Adobe)</li>
            <li>Elisa Ricci (University of Trento)</li>
            <li>BjÃ¶rn Ommer (University of Munich)</li>
            <li>Ohad Fried (The Interdisciplinary Center Herzliya)</li>
            <li>David Bau (Northeastern)</li>
            <li>Xun Huang (NVIDIA)</li>
        </ul>
    </div>
    -->

    <!--
    <div class="container" id="competitions">
        <hr>
        <h3>Competitions</h3>

        <p>
        We are planning an AI-based design competition and an AI-based image generation competition. More details coming soon.
        </p>
    </div>
    -->

    <!--
    <div class="container" id="submission">
        <hr>
        <h3>Submission Instructions</h3>
        <p>
            We call for papers (8 pages) and extended abstracts (4 pages not including references) to be presented at the AI for Content Creation Workshop at CVPR 2022. Papers and extended abstracts will be peer reviewed in a double blind fashion. Authors of accepted papers will be asked to post their submissions on arXiv. These papers are not archival and will not be included in the proceedings of CVPR 2022, but authors should be aware that computer vision conferences consider peer-reviewed works with >4-pages to be in violation of double submission policies, e.g., both <a href="http://cvpr2022.thecvf.com/node/33#policies">CVPR</a> and <a href="https://eccv2020.eu/author-instructions/">ECCV</a>. We welcome both novel work and work in progress that have not been published elsewhere. 
        </p>

        <p>
            In the interests of fostering a free exchange of ideas, we will also accept for presentation a selection of papers that have been recently published elsewhere, including at CVPR 2022; these will not be peer reviewed again, and are not bound to the same anonymity and page limits. A jury of organizers will select these papers.
        </p>
        
        <p>
        Paper submissions for 4- and 8-page novel work are <em>double blind</em> and in the <a href="http://cvpr2022.thecvf.com/submission/main-conference/author-guidelines#submission-guidelines">CVPR template</a>. There are <em>no dual submissions</em>&mdash;please do not submit work for peer review to two workshops simultaneously.
        </p>

        <p>
        Paper submission deadline: March 18th 2022 23:59 PST<br>
        Acceptance notification: ~April 20th 2022<br>
        Submission Website: <a href="https://cmt3.research.microsoft.com/AI4CC2022/">https://cmt3.research.microsoft.com/AI4CC2022/</a>
        </p>

        <p>
        The best student papers will be acknowledged with a prize; Adobe Creative Cloud subscriptions, a cash award, or an NVIDIA GPU.
        </p>

        <p>
        We seek contributions across content creation, including but not limited to techniques for content creation:
        </p>
        <ul>
            <li>Generative models for image/video synthesis</li>
            <li>Image/video editing, inpainting, or extrapolation</li>
            <li>Image and video stylization and translation</li>
            <li>Text-to-image creation</li>
        </ul>
        
        <p>
        We also seek contributions in domains and applications for content creation:
        </p>
        <ul>
            <li>Image and video synthesis for enthusiast, VFX, architecture, advertisements, art, ...</li>
            <li>2D/3D graphic design</li>
            <li>Text and typefaces</li>
            <li>Design for documents, Web</li>
            <li>Fashion, garments, and outfits</li>
            <li>Novel applications and datasets</li>
        </ul>

        <br><br>
    </div>
    -->

    

    <div class="container" id="previousworkshops">
        <hr>    
        <h3>Previous Workshops</h3>
        <ul>
            <li>2021 - <a href="./2021/">AI for Content Creation</a> (Workshop at CVPR 2021). <br>Includes session video recordings!</li>
            <li>2020 - <a href="./2020/">AI for Content Creation</a> (Workshop at CVPR 2020). <br>Includes session video recordings!</li>
            <li>2019 - <a href="https://nvlabs.github.io/dl-for-content-creation/">Deep Learning for Content Creation</a> (Tutorial at CVPR 2019)</li>
        </ul>
        <br><br>
    </div>

    <footer class="bg-light text-dark py-5">
        <div class="container">
            <img src="images/logos/google_logo_tp.png" class="logo">
            <img src="images/logos/brown-cs-logo.png" class="logo">
            <img src="images/logos/NVIDIALogo_2D.png" class="logo">
            <img src="images/logos/Adobe-Logo.png" class="logo">
            <img src="images/logos/berkeley-logo.png" class="logo">
            <img src="images/logos/U-of-T-logo_blue.svg" class="logo">

            <p>
                Thank you to <a href="http://themestr.app">Themestr.app</a> and <a href="http://getbootstrap.com">Bootstrap</a> for the Webpage design tools.
            </p>
        </div>
    </footer>

    <!-- Javascript to control YouTube player-->
    <!--
    <script>
        // From https://developers.google.com/youtube/iframe_api_reference
        document.addEventListener("DOMContentLoaded", function(event) {
            var tag = document.createElement('script');
            tag.src = "https://www.youtube.com/iframe_api";
            var firstScriptTag = document.getElementsByTagName('script')[0];
            firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
        });

        var morningplayer, afternoonplayer;
        function onYouTubeIframeAPIReady() {
            morningplayer = new YT.Player('morningplayer', {width: '800', height: '450', videoId: 'x6cHOulDXUo'});
            afternoonplayer = new YT.Player('afternoonplayer', {width: '800', height: '450', videoId: 'DJzMLa8zuJQ'});
        }
    </script>
    -->
  </body>
</html>
