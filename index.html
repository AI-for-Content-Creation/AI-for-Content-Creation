<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <link href="css/theme_1610153848925.css" rel="stylesheet"> <!-- Via Themestr.app -->

    <!-- Option 1: Bootstrap Bundle with Popper -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>

    <style>
    p {
        text-align: justify;
        -webkit-hyphens: auto;
            -moz-hyphens: auto;
        hyphens: auto;
    }

    /* 
    .brownbrown {
        color: #4E3629;
    }

    .brownred {
        color: #C00404;
    } */

    .logo {
        width: 20em;
        padding: 1em 1em 1em 1em;
    }
    </style>


  <title>AICC 2022</title>
  </head>

  <body>
    
    <header class="bg-light text-dark py-5">
    <div class="container text-center">
        <h1>AI for Content Creation Workshop</h1>
        <h3>2022</h3>
        <h4>Date TBD</h4>
    </div>
    </header>

    <!-- A grey horizontal navbar that becomes vertical on small screens -->
    <nav class="navbar sticky-top navbar-expand-sm" style="margin-top:2em; background: #FFF;">
        <div class="container">
            <a class="navbar-brand" href="#">AICC 2022</a>
            
            <ul class="navbar-nav">
            <li class="nav-item">
                <a class="nav-link link-primary" href="#summary">Summary</a>
            </li>
            <li class="nav-item">
                <a class="nav-link link-primary" href="#videorecording">Video Recording</a>
            </li>
            <li class="nav-item">
                <a class="nav-link link-primary" href="#awards">Awards</a>
            </li>
            <li class="nav-item">
                <a class="nav-link link-primary" href="#papers">Papers</a>
            </li>
            <!--
            <li class="nav-item">
                <a class="nav-link link-primary" href="#submission">Submission</a>
            </li>
            -->
            <li class="nav-item">
                <a class="nav-link link-primary" href="#previousworkshops">Previous Workshops</a>
            </li>    
            </ul>
        </div>
    </nav>


    <div class="container" style="margin-top:2em">
        <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
            <ol class="carousel-indicators">
                <li data-target="#carouselExampleIndicators" data-slide-to="0" class="active"></li>
                <li data-target="#carouselExampleIndicators" data-slide-to="1"></li>
                <li data-target="#carouselExampleIndicators" data-slide-to="2"></li>
            </ol>

            <div class="carousel-inner">
                <div class="carousel-item text-center active">
                    <img height="300px" src="images/rombach_aicc2020_network-fusion_exemplarguided_crop.jpg" alt="Network Fusion for Content Creation with Conditional INNs">
                    <div class="carousel-caption d-none d-md-block">
                    <h5 class="text-dark text-center" style="background: #FFF;">Rombach, Esser, and Ommer, AICC 2020</h5>
                    <p class="text-dark text-center" style="background: #FFF;">Network Fusion for Content Creation with Conditional INNs</p>
                    </div>
                </div>

                <div class="carousel-item text-center" style="height: 300px">
                    <img width="100%" src="images/Cha_aicc2020_few-shot-font-generation.jpg" alt="Toward High-quality Few-shot Font Generation with Dual Memory">
                    <div class="carousel-caption d-none d-md-block">
                    <h5 class="text-dark text-center" style="background: #FFF;">Cha et al., AICC 2020</h5>
                    <p class="text-dark text-center" style="background: #FFF;">Toward High-quality Few-shot Font Generation with Dual Memory</p>
                    </div>
                </div>

                <div class="carousel-item text-center">
                    <img height="300px" src="images/sylvain_aicc2020_object-centric_image_generation.jpg" alt="Object-Centric Image Generation from Layouts">
                    <div class="carousel-caption d-none d-md-block">
                    <h5 class="text-dark text-center" style="background: #FFF;">Sylvain et al., AICC 2020</h5>
                    <p class="text-dark text-center" style="background: #FFF;">Object-Centric Image Generation from Layouts</p>
                    </div>
                </div>
            </div>

            <a class="carousel-control-prev" href="#carouselExampleIndicators" role="button" data-slide="prev">
                <span class="carousel-control-prev-icon" aria-hidden="true"></span>
                <span class="sr-only">Previous</span>
            </a>
            <a class="carousel-control-next" href="#carouselExampleIndicators" role="button" data-slide="next">
                <span class="carousel-control-next-icon" aria-hidden="true"></span>
                <span class="sr-only">Next</span>
            </a>
        </div>
        <br><br>
    </div>


    <div class="container" id="summary">
        <h3>Summary</h3>
        <p>
        The AI for Content Creation (AICC) workshop at CVPR 2022 brings together researchers in computer vision, machine learning, and AI. Content creation has several important applications ranging from virtual reality, videography, gaming, and even retail and advertising. The recent progress of deep learning and machine learning techniques allowed to turn hours of manual, painstaking content creation work into minutes or seconds of automated work. For instance, generative adversarial networks (GANs) have been used to produce photorealistic images of items such as shoes, bags, and other articles of clothing, interior/industrial design, and even computer games' scenes. Neural networks can create impressive and accurate slow-motion sequences from videos captured at standard frame rates, thus side-stepping the need for specialized and expensive hardware. Style transfer algorithms can convincingly render the content of one image with the style of another, offering unique opportunities for generating additional and more diverse training data---in addition to creating awe-inspiring, artistic images.
        Learned priors can also be combined with explicit geometric constraints, allowing for realistic and visually pleasing solutions to traditional problems such as novel view synthesis, in particular for the more complex cases of view extrapolation.
        </p>

        <p>
        AI for content creation lies at the intersection of the graphics, the computer vision, and the design community. However, researchers and professionals in these fields may not be aware of its full potential and inner workings. As such, the workshop is comprised of two parts: techniques for content creation and applications for content creation. The workshop has three goals:
        </p>
        <ol>
            <li>To cover introductory concepts to help interested researchers from other fields start in this exciting area.</li>
            <li>To present success stories to show how deep learning can be used for content creation.</li>
            <li>To discuss pain points that designers face using content creation tools.</li>
        </ol>
        <p>
        More broadly, we hope that the workshop will serve as a forum to discuss the latest topics in content creation and the challenges that vision and learning researchers can help solve.
        </p>

        <p>
        Welcome! - <br>
        <a class="link-primary" href="https://deqings.github.io/">Deqing Sun (Google)</a>, <a href="https://twitter.com/DeqingSun"><img src="images/logos/Twitter_Logo.png" width=18px> <br>
        <a class="link-primary" href="https://www.linkedin.com/in/huiwen-chang-999962156/">Huiwen Chang (Google)</a> <br>
        <a class="link-primary" href="http://www.lujiang.info/">Lu Jiang (Google)</a>, <a href="https://twitter.com/roadjiang"><img src="images/logos/Twitter_Logo.png" width=18px> <br>
        <a class="link-primary" href="https://liaojing.github.io/html/">Liao Jing (City University of Hong Kong)</a> <br>
        <a class="link-primary" href="https://research.nvidia.com/person/mingyu-liu">Ming-Yu Liu (NVIDIA)</a> <a href="https://twitter.com/liu_mingyu"><img src="images/logos/Twitter_Logo.png" width=18px> <br>
        <a class="link-primary" href="https://research.adobe.com/person/jingwan-lu">Cynthia Lu (Adobe)</a> <a href="https://twitter.com/JingwanL"><img src="images/logos/Twitter_Logo.png" width=18px> <br>
        <a class="link-primary" href="https://seungjunnah.github.io/">Seungjan Nah (NVIDIA)</a> <br>
        <a class="link-primary" href="http://www.kalyans.org/">Kalyan Sunkavalli (Adobe)</a> <br> 
        <a class="link-primary" href="http://www.jamestompkin.com">James Tompkin (Brown)</a> <a href="https://twitter.com/jtompkin"><img src="images/logos/Twitter_Logo.png" width=18px> 
        <a class="link-primary" href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu (Carnegie Mellon)</a> <br>
            
        <!--
        Steering committee
        
        <a class="link-primary" href="https://www.cs.utoronto.ca/~fidler">Sanja Fidler (UToronto / NVIDIA)</a>, <a href="https://twitter.com/FidlerSanja"><img src="images/logos/Twitter_Logo.png" width=18px> <br>    
        <a class="link-primary" href="https://people.eecs.berkeley.edu/~kanazawa">Angjoo Kanazawa (UC Berkeley)</a>, <a href="https://twitter.com/akanazawa"><img src="images/logos/Twitter_Logo.png" width=18px> <br>
        <a class="link-primary" href="https://research.google/people/107064/">Weilong Yang (Waymo)</a>. <br> 
        -->
            
        </p>
    </div>

    <div class="container text-center">
        <img src="images/superslowmo.jpg" width=25% style="padding: 1em 0em 1em 0em">
        <img src="images/dogs.jpg" width=34.5% style="padding: 1em 0em 1em 0em">
        <img src="images/Waterfill-capture_blog-crop-1280x680.jpg" width=25.5% style="padding: 1em 0em 1em 0em">
        <br><br>
    </div>

    <div class="container">
        <h3 id="videorecording">Video Recording <a href="https://www.youtube.com/watch?v=x6cHOulDXUo&list=PLNPyQ_mnkEr7ae_dbhv4MG2ja5EI74lzu">(YouTube Playlist)</a></h3>
        
        Morning session:<br>
        <div id="morningplayer"></div> <!--This will be replaced by an iFrame video player in a script at the bottom of the source.-->
        <p><em>Click ▶ to jump to each talk!</em></p>

        <table class="table table-sm table-striped">
            <thead>
                <tr>
                    <th>Play</th><th scope="col">Time PDT</th><th scope="col">Repeat viewing</th><th scope="col"></td>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td></td>
                    <td>08:00</td>
                    <td>20:00</td>
                    <td>Welcome and introductions</td>
                    <td></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.loadVideoById('x6cHOulDXUo')">▶</button></td>
                    <td>08:15</td>
                    <td>20:15</td>
                    <td><a href="https://www.cs.utexas.edu/users/grauman/">Kristen Grauman (UT Austin)</a></td>
                    <td></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.loadVideoById('FTsW29ZcovM')">▶</button></td>
                    <td>08:45</td>
                    <td>20:45</td>
                    <td><a href="https://www.niessnerlab.org/">Matthias Nießner (TU Munich)</a></td>
                    <td><a href="https://twitter.com/MattNiessner"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.loadVideoById('032X8G-Zk6Y')">▶</button></td>
                    <td>09:15</td>
                    <td>21:15</td>
                    <td><a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman (University of Washington)</a></td>
                    <td><a href="https://twitter.com/kemelmi"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.loadVideoById('s2NPUUug4Yc')">▶</button></td>
                    <td>10:00</td>
                    <td>22:00</td>
                    <td><a href="https://www.weizmann.ac.il/math/dekel/">Tali Dekel (Weizmann Institute of Science)</a></td> 
                    <td><a href="http://twitter.com/talidekel"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.loadVideoById('OGURjRt6f3g')">▶</button></td>
                    <td>10:30</td>
                    <td>22:30</td>
                    <td><a href="http://www.cs.toronto.edu/~urtasun">Raquel Urtasun (Waabi, University of Toronto)</a></td>
                    <td><a href="https://twitter.com/RaquelUrtasun"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.loadVideoById('e-JPKe3ySZY')">▶</button></td>
                    <td>11:00</td>
                    <td>23:00</td>
                    <td>Oral session 1: Kips et al., <a href="https://arxiv.org/abs/2105.06407">Deep Graphics Encoder for Real Time Video Makeup Synthesis from Example</a></td>    
                    <td></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="morningplayer.loadVideoById('e-JPKe3ySZY',764)">▶</button></td>
                    <td>11:10</td>
                    <td>23:10</td>
                    <td>Oral session 1: Mejjati et al., <a href="https://visual.cs.brown.edu/gaussigan">GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes</a></td>    
                    <td></td>
                </tr>
                <tr>
                    <td></td>
                    <td>11:20</td>
                    <td>23:20</td>
                    <td>Poster session 1
                        <ul>
                            <li>Agrawal et al., <a href="https://arxiv.org/abs/2105.05712">Directional GAN: A Novel Conditioning Strategy for Generative Networks</a></li>
                            <li>Sushko et al., <a href="https://arxiv.org/abs/2105.05847">Learning to Generate Novel Scene Compositions from Single Images and Videos</a></li>
                            <li>Zhang et al., <a href="http://arxiv.org/abs/2105.08222">Decorating Your Own Bedroom: Locally Controlling Image Generation with Generative Adversarial Networks</a></li>
                            <li>Ruiz et al., <a href="https://arxiv.org/abs/2012.05225">MorphGAN: Controllable One-Shot Face Synthesis</a></li>
                            <li>Park et al., <a href="https://github.com/clovaai/lffont">Few-shot Font Generation with Localized Style Representations and Factorization</a></li>
                            <li>Kim et al., <a href="https://github.com/naver-ai/StyleMapGAN">Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing</a></li>
                            <li>Kulkarni et al., <a href="https://arxiv.org/abs/2004.08614">Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships</a></li>
                            <li>Kips et al., <a href="https://arxiv.org/abs/2105.06407">Deep Graphics Encoder for Real Time Video Makeup Synthesis from Example</a></li>
                            <li>Mejjati et al., <a href="https://visual.cs.brown.edu/gaussigan">GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes</a></li>
                        </ul>
                    </td>    
                    <td></td>
                </tr>
            </tbody>
        </table>


        <br><br>
        Afternoon session:<br>
        <div id="afternoonplayer"></div> <!--This will be replaced by an iFrame video player in a script at the bottom of the source.-->
        <p><em>Click ▶ to jump to each talk!</em></p>

        <table class="table table-sm table-striped">
            <thead>
                <tr>
                    <th>Play</th><th scope="col">Time PDT</th><th scope="col">Repeat viewing</th><th scope="col"></td>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.loadVideoById('DJzMLa8zuJQ')">▶</button></td>
                    <td>13:00</td>
                    <td>01:00 (26th)</td>
                    <td><a href="http://www.stulyakov.com/">Sergey Tulyakov (Snap Research)</a></td>
                    <td><a href="https://twitter.com/SergeyTulyakov"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.loadVideoById('xHTOwuc32ow')">▶</button></td>
                    <td>13:30</td>
                    <td>01:30 (26th)</td>
                    <td><a href="https://www.cc.gatech.edu/~parikh">Devi Parikh (Georgia Tech)</a></td>
                    <td><a href="https://twitter.com/deviparikh"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.loadVideoById('RUA4CKiKSic')">▶</button></td>
                    <td>14:00</td>
                    <td>02:00 (26th)</td>
                    <td><a href="https://cephaloponderer.com/">Emily Denton (Google)</a></td>
                    <td><a href="https://twitter.com/cephaloponderer"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.loadVideoById('ZsGEAr_inOQ')">▶</button></td>
                    <td>14:30</td>
                    <td>02:30 (26th)</td>
                    <td><a href="https://jonbarron.info/">Jon Barron (Google)</a></td>
                    <td><a href="https://twitter.com/jon_barron"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.loadVideoById('kAkGNpaR9Iw')">▶</button></td>
                    <td>15:00</td>
                    <td>03:00 (26th)</td>
                    <td>Oral session 2: Jahn et al., <a href="https://arxiv.org/abs/2105.06458">High-Resolution Complex Scene Synthesis with Transformers</a></td>
                    <td></td>
                </tr>
                <tr>
                    <td><button type="button" class="btn btn-outline-secondary btn-sm" onclick="afternoonplayer.loadVideoById('kAkGNpaR9Iw',917)">▶</button></td>
                    <td>15:10</td>
                    <td>03:10 (26th)</td>
                    <td>Oral session 2: Mordvintsev et al., <a href="https://arxiv.org/abs/2105.07299">Texture Generation with Neural Cellular Automata</a></td>    
                    <td></td>
                </tr>
                <tr>
                    <td></td>
                    <td>15:20</td>
                    <td>03:20 (26th) </td>
                    <td>Poster session 2
                        <ul>
                            <li>Narasimhan et al., <a href="https://drive.google.com/file/d/1DDZHXTrGFWixbwM4Rvf7Kfrze0pzlIYc/view?usp=sharing">Strumming to the Beat: Audio-Conditioned Contrastive Video Textures</a></li>
                            <li>Frey et al., <a href="https://arxiv.org/abs/2105.06988">Automatic Style Transfer for Non-Linear Video Editing</a></li>
                            <li>Zhang, Thamizharasan et al., <a href="https://drive.google.com/file/d/1oie_epsUj0Eygm5pKz4DNg7mx99ETgAo/view?usp=sharing">Learning Physically-based Face Material and Lighting Decompositions</a></li>
                            <li>Tseng et al., <a href="https://hytseng0509.github.io/lecam-gan/">Regularizing Generative Adversarial Networks under Limited Data</a></li>
                            <li>Xu et al., <a href="https://github.com/genforce/ghfeat">Generative Hierarchical Features from Synthesizing Images</a></li>
                            <li>Jahn et al., <a href="https://arxiv.org/abs/2105.06458">High-Resolution Complex Scene Synthesis with Transformers</a></li>
                            <li>Mordvintsev et al., <a href="https://arxiv.org/abs/2105.07299">Texture Generation with Neural Cellular Automata</a></li>
                        </ul>
                    </td>
                    <td></td>
                </tr>
            </tbody>
        </table>

    </div>

    <!-- <div class="container">
    <hr>
    <h3 id="schedule">Schedule</h3>
        <table class="table table-sm table-striped">
        <thead>
            <tr>
                <th scope="col">Time PDT</th><th scope="col">Repeat viewing</th><th scope="col"></td>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>08:00</td>
                <td>20:00</td>
                <td>Welcome and introductions</td>
                <td></td>
            </tr>
            <tr>
                <td>08:15</td>
                <td>20:15</td>
                <td><a href="https://www.cs.utexas.edu/users/grauman/">Kristen Grauman (UT Austin)</a></td>
                <td></td>
            </tr>
            <tr>
                <td>08:45</td>
                <td>20:45</td>
                <td><a href="https://www.niessnerlab.org/">Matthias Nießner (TU Munich)</a></td>
                <td><a href="https://twitter.com/MattNiessner"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
            </tr>
            <tr>
                <td>09:15</td>
                <td>21:15</td>
                <td><a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman (University of Washington)</a></td>
                <td><a href="https://twitter.com/kemelmi"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
            </tr>
            <tr>
                <td>09:45</td>
                <td>21:45</td>
                <td>Coffee break</td>
                <td></td>
            </tr>
            <tr>
                <td>10:00</td>
                <td>22:00</td>
                <td><a href="https://www.weizmann.ac.il/math/dekel/">Tali Dekel (Weizmann Institute of Science)</a></td> 
                <td><a href="http://twitter.com/talidekel"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
            </tr>
            <tr>
                <td>10:30</td>
                <td>22:30</td>
                <td><a href="http://www.cs.toronto.edu/~urtasun">Raquel Urtasun (Waabi, University of Toronto)</a></td>
                <td><a href="https://twitter.com/RaquelUrtasun"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
            </tr>
            <tr>
                <td>11:00</td>
                <td>23:00</td>
                <td>Oral session 1: Kips et al., <a href="https://arxiv.org/abs/2105.06407">Deep Graphics Encoder for Real Time Video Makeup Synthesis from Example</a></td>    
                <td></td>
            </tr>
            <tr>
                <td>11:10</td>
                <td>23:10</td>
                <td>Oral session 1: Mejjati et al., <a href="https://visual.cs.brown.edu/gaussigan">GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes</a></td>    
                <td></td>
            </tr>
            <tr>
                <td>11:20</td>
                <td>23:20</td>
                <td>Poster session 1
                    <ol>
                        <li>Agrawal et al., <a href="https://arxiv.org/abs/2105.05712">Directional GAN: A Novel Conditioning Strategy for Generative Networks</a></li>
                        <li>Sushko et al., <a href="https://arxiv.org/abs/2105.05847">Learning to Generate Novel Scene Compositions from Single Images and Videos</a></li>
                        <li>Zhang et al., <a href="http://arxiv.org/abs/2105.08222">Decorating Your Own Bedroom: Locally Controlling Image Generation with Generative Adversarial Networks</a></li>
                        <li>Ruiz et al., <a href="https://arxiv.org/abs/2012.05225">MorphGAN: Controllable One-Shot Face Synthesis</a></li>
                        <li>Park et al., <a href="https://github.com/clovaai/lffont">Few-shot Font Generation with Localized Style Representations and Factorization</a></li>
                        <li>Kim et al., <a href="https://github.com/naver-ai/StyleMapGAN">Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing</a></li>
                        <li>Kulkarni et al., <a href="https://arxiv.org/abs/2004.08614">Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships</a></li>
                        <li>Kips et al., <a href="https://arxiv.org/abs/2105.06407">Deep Graphics Encoder for Real Time Video Makeup Synthesis from Example</a></li>
                        <li>Mejjati et al., <a href="https://visual.cs.brown.edu/gaussigan">GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes</a></li>
                    </ol>
                </td>    
                <td></td>
            </tr>
            <tr>
                <td>12:00</td>
                <td>00:00 (26th)</td>
                <td>Lunch</td>    
                <td></td>
            </tr>
            <tr>
                <td>13:00</td>
                <td>01:00 (26th)</td>
                <td><a href="http://www.stulyakov.com/">Sergey Tulyakov (Snap Research)</a></td>
                <td><a href="https://twitter.com/SergeyTulyakov"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
            </tr>
            <tr>
                <td>13:30</td>
                <td>01:30 (26th)</td>
                <td><a href="https://www.cc.gatech.edu/~parikh">Devi Parikh (Georgia Tech)</a></td>
                <td><a href="https://twitter.com/deviparikh"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
            </tr>
            <tr>
                <td>14:00</td>
                <td>02:00 (26th)</td>
                <td><a href="https://cephaloponderer.com/">Emily Denton (Google)</a></td>
                <td><a href="https://twitter.com/cephaloponderer"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
            </tr>
            <tr>
                <td>14:30</td>
                <td>02:30 (26th)</td>
                <td><a href="https://jonbarron.info/">Jon Barron (Google)</a></td>
                <td><a href="https://twitter.com/jon_barron"><img src="images/logos/Twitter_Logo.png" width=18px></a></td>
            </tr>
            <tr>
                <td>15:00</td>
                <td>03:00 (26th)</td>
                <td>Oral session 2: Jahn et al., <a href="https://arxiv.org/abs/2105.06458">High-Resolution Complex Scene Synthesis with Transformers</a></td>
                <td></td>
            </tr>
            <tr>
                <td>15:10</td>
                <td>03:10 (26th)</td>
                <td>Oral session 2: Mordvintsev et al., <a href="https://arxiv.org/abs/2105.07299">Texture Generation with Neural Cellular Automata</a></td>    
                <td></td>
            </tr>
            <tr>
                <td>15:20</td>
                <td>03:20 (26th) </td>
                <td>Poster session 2
                    <ol>
                        <li>Narasimhan et al., <a href="https://drive.google.com/file/d/1DDZHXTrGFWixbwM4Rvf7Kfrze0pzlIYc/view?usp=sharing">Strumming to the Beat: Audio-Conditioned Contrastive Video Textures</a></li>
                        <li>Frey et al., <a href="https://arxiv.org/abs/2105.06988">Automatic Style Transfer for Non-Linear Video Editing</a></li>
                        <li>Zhang, Thamizharasan et al., <a href="https://drive.google.com/file/d/1oie_epsUj0Eygm5pKz4DNg7mx99ETgAo/view?usp=sharing">Learning Physically-based Face Material and Lighting Decompositions</a></li>
                        <li>Tseng et al., <a href="https://hytseng0509.github.io/lecam-gan/">Regularizing Generative Adversarial Networks under Limited Data</a></li>
                        <li>Xu et al., <a href="https://github.com/genforce/ghfeat">Generative Hierarchical Features from Synthesizing Images</a></li>
                        <li>Jahn et al., <a href="https://arxiv.org/abs/2105.06458">High-Resolution Complex Scene Synthesis with Transformers</a></li>
                        <li>Mordvintsev et al., <a href="https://arxiv.org/abs/2105.07299">Texture Generation with Neural Cellular Automata</a></li>
                    </ol>
                </td>
                <td></td>
            </tr>
            <tr>
                <td>16:00</td>
                <td>04:00 (26th)</td>
                <td>Best paper announcement and closing statements</td>
                <td></td>
            </tr>
        </tbody>
        </table>
    </div> -->

    <!--
    <div class="container w-75">
        <hr>
        <h3 id="awards">Awards</h3>
        <h4>Best paper</h4>
        <ul>
            <li>High-Resolution Complex Scene Synthesis with Transformers<br>
                <em>Manuel Jahn (Heidelberg University); Robin Rombach (Heidelberg University); Bjorn Ommer (Heidelberg University)</em><br>
                [<a href="https://arxiv.org/abs/2105.06458">arXiv</a>]
            </li>
        </ul>
    </div>


    <div class="container w-75">
        <hr>
        <h3 id="papers">All accepted works (in random order)</h3>

        <h4>Papers (8 pages)</h4>
        <ul>
            <li>Directional GAN: A Novel Conditioning Strategy for Generative Networks<br>
                <em>Shradha Agrawal (Adobe Inc); Shankar Venkitachalam (Adobe Inc); Dhanya Raghu (USC); Deepak Pai (Adobe)</em><br>
                [<a href="https://arxiv.org/abs/2105.05712">arXiv</a>]
            </li>
            <li>Texture Generation with Neural Cellular Automata<br>
                <em>Alexander Mordvintsev (Google Switzerland GmbH); Eyvind Niklasson (Google Switzerland GmbH); Ettore Randazzo (Google Switzerland GmbH)</em><br>
                [<a href="https://arxiv.org/abs/2105.07299">arXiv</a>] [<a href="https://selforglive.github.io/cvpr_textures/">Webpage (Interactive Demo)</a>]
            </li>
            <li>Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships<br>
                <em>Kuldeep Kulkarni (Adobe Research); Tejas Gokhale (Arizona State University); Rajhans Singh (Arizona State University); Pavan Turaga (Arizona State University); Aswin Sankaranarayanan (Carnegie Mellon University)</em><br>
                [<a href="https://arxiv.org/abs/2004.08614">arXiv</a>]
            </li>

        </ul>        

        <h4>Extended abstracts (4 pages)</h4>
        <ul>
            <li>Deep Graphics Encoder for Real Time Video Makeup Synthesis From Example<br>
                <em>Robin KIPS (L'Oreal); Ruowei Jiang (ModiFace Inc.); Sileye BA (L'Oreal); Edmund Phung (ModiFace Inc); Parham Aarabi (ModiFace Inc.); Pietro Gori (Télécom Paris); Matthieu PERROT (L'Oreal); Isabelle Bloch (Télécom Paris)</em><br>
                [<a href="https://arxiv.org/abs/2105.06407">arXiv</a>] [<a href="https://drive.google.com/file/d/1oOBtj9unIgQ_LD88DGV-DgywonsvmDCW/view?usp=sharing">Supplemental PDF</a>]
            </li>
            <li>Strumming to the Beat: Audio-Conditioned Contrastive Video Textures<br>
                <em>Medhini Narasimhan (UC Berkeley); Shiry Ginosar (UC Berkeley); Andrew Owens (U Michigan); Alexei A Efros (UC Berkeley); Trevor Darrell (UC Berkeley)</em><br>
                [<a href="https://drive.google.com/file/d/1DDZHXTrGFWixbwM4Rvf7Kfrze0pzlIYc/view?usp=sharing">PDF (4-page version; peer reviewed)</a>] [<a href="https://arxiv.org/abs/2104.02687">arXiv (8-page version; not peer reviewed)</a>]
            </li>
            <li>High-Resolution Complex Scene Synthesis with Transformers<br>
                <em>Manuel Jahn (Heidelberg University); Robin Rombach (Heidelberg University); Bjorn Ommer (Heidelberg University)</em><br>
                [<a href="https://arxiv.org/abs/2105.06458">arXiv</a>]
            </li>
            <li>Learning to Generate Novel Scene Compositions from Single Images and Videos<br>
                <em>Vadim Sushko (Bosch Center for Artificial Intelligence); Jürgen Gall (University of Bonn); Anna Khoreva (Bosch Center for Artificial Intelligence)</em><br>
                [<a href="https://arxiv.org/abs/2105.05847">arXiv</a>]
            </li>
            <li>Automatic Style Transfer for Non-Linear Video Editing<br>
                <em>Nathan Frey (Google Research); Peggy Chi (Google Research); Weilong Yang (Google Inc.); Irfan Essa (Google)</em><br>
                [<a href="https://arxiv.org/abs/2105.06988">arXiv</a>]
            </li>
            <li>Decorating Your Own Bedroom: Locally Controlling Image Generation with Generative Adversarial Networks<br>
                <em>Chen Zhang (Zhejiang University); Yinghao Xu (Chinese University of Hong Kong); Yujun Shen (Dept. of IE, CUHK)</em><br>
                [<a href="http://arxiv.org/abs/2105.08222">arXiv</a>]
            </li>
            <li>MorphGAN: Controllable One-Shot Face Synthesis<br>
                <em>Nataniel Ruiz (Boston University); Barry Theobald (Apple); Anurag Ranjan (Apple); Ahmed Hussein Abdelaziz (Apple); Nicholas Apostoloff (Apple Inc.)</em><br>
                [<a href="https://arxiv.org/abs/2012.05225">arXiv</a>]
            </li>
            <li>GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes<br>
                <em>Youssef Alami Mejjati (University of Bath); Isa Milefchik (Brown University); Aaron K Gokaslan (Cornell); Oliver Wang (Adobe Systems Inc); Kwang In Kim (UNIST); James Tompkin (Brown University)</em><br>
                [<a href="https://visual.cs.brown.edu/gaussigan">Webpage</a>]
            </li>
            <li>Learning Physically-based Face Material and Lighting Decompositions<br>
                <em>Qian Zhang (Brown University)*; Vikas Thamizharasan (Brown University)*; James Tompkin (Brown University)</em><br>
                [<a href="https://drive.google.com/file/d/1oie_epsUj0Eygm5pKz4DNg7mx99ETgAo/view?usp=sharing">PDF</a>]
            </li>
        </ul>

        <h4>Papers (8 pages)&mdash;also in other proceedings</h4>
        <ul>
            <li>Few-shot Font Generation with Localized Style Representations and Factorization<br>
                <em>Song Park (Yonsei University); Sanghyuk Chun (NAVER AI LAB); Junbum Cha (Clova AI Research, NAVER Corp.); Bado Lee (Clova AI Research, NAVER Corp.); Hyunjung Shim (Yonsei University)</em><br>
                AAAI 2021<br>
                [<a href="https://arxiv.org/abs/2009.11042">arXiv</a>] [<a href="https://github.com/clovaai/lffont">Webpage</a>]
            </li>
            <li>Regularizing Generative Adversarial Networks under Limited Data<br>
                <em>Hung-Yu Tseng (University of California, Merced); Lu Jiang (Google Research); Ce Liu (Google); Ming-Hsuan Yang (University of California at Merced); Weilong Yang (Google Inc.)</em><br>
                CVPR 2021<br>
                [<a href="https://arxiv.org/abs/2104.03310">arXiv</a>] [<a href="https://hytseng0509.github.io/lecam-gan/">Webpage</a>]
            </li>
            <li>Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing<br>
                <em>Hyunsu Kim (NAVER AI Lab); Yunjey Choi (NAVER AI Lab); Junho Kim (NAVER AI Lab); Sungjoo Yoo (Seoul National University); Youngjung Uh (Yonsei University)</em><br>
                CVPR 2021<br>
                [<a href="https://arxiv.org/abs/2104.14754">arXiv</a>] [<a href="https://github.com/naver-ai/StyleMapGAN">Webpage</a>]
            </li>
            <li>Generative Hierarchical Features from Synthesizing Images<br>
                <em>Yinghao Xu (Chinese University of Hong Kong); Yujun Shen (Dept. of IE, CUHK); Jiapeng Zhu (HKUST); Ceyuan Yang (Chinese University of Hong Kong); Bolei Zhou (CUHK)</em><br>
                CVPR 2021<br>
                [<a href="https://arxiv.org/abs/2007.10379">arXiv</a>] [<a href="https://github.com/genforce/ghfeat">Webpage</a>]
            </li>
        </ul>
    </div>
    -->


    <div class="container" id="submission">
        <hr>
        <h3>Submission Instructions</h3>
        <p>
        We call for papers (8 pages not including references) and extended abstracts (4 pages not including references) to be showcased in a poster session, and for interactive demos, both for the AI for Content Creation Workshop at CVPR 2022. Authors of accepted papers and extended abstracts will be asked to post their submissions on arXiv. Both papers and extended abstracts will not be included in the proceedings of CVPR 2022, but authors should be aware that computer vision conferences consider peer-reviewed works with >4-pages to be in violation of double submission policies, e.g., both <a href="http://cvpr2022.thecvf.com/node/33#policies">CVPR</a> and <a href="https://eccv2020.eu/author-instructions/">ECCV</a>. We will accept work in progress, work that has not been published elsewhere, and work that has been recently published elsewhere including at CVPR 2022. In the interests of fostering a free exchange of ideas, we welcome both novel and previously-published work.
        </p>
        
        <p>
        Paper submissions are <em>double blind</em> and in the <a href="http://cvpr2022.thecvf.com/submission/main-conference/author-guidelines#submission-guidelines">CVPR template</a>.
        </p>

        <p>
        Paper submission deadline: tentatively March 18th 2022 11:59 PST<br>
        Acceptance notification: mid/late April<br>
        Submission Website (coming soon): <a href="https://cmt3.research.microsoft.com/AICC2022/">https://cmt3.research.microsoft.com/AICC2022/</a>
        </p>

        <p>
        The best student paper will be acknowledged with a prize of an NVIDIA GPU (kindly provided by our sponsors).
        </p>

        <p>
        We seek contributions across content creation, including but not limited to:
        </p>
        <ul>
            <li>Image and video creation and editing, visual effects</li>
            <li>2D/3D graphic design</li>
            <li>Text and typefaces</li>
            <li>Design for documents, Web, print, advertising</li>
            <li>Fashion, garments, and outfits</li>
            <li>Art and architecture</li>
            <li>Fairness and ethics in AI-based content creation</li>
            <li>Novel applications and datasets</li>
            <li>...</li>
        </ul>
            
        <p>
        This includes domains and applications for content creation:
        </p>    
        

        <br><br>
    </div>
    
    <div class="container" id="previousworkshops">
        <h4>Previous Workshops</h4>
        <ul>
            <li>2021 - <a href="./2021/">AI for Content Creation</a> (Workshop at CVPR 2021). Includes session video recordings!</li>
            <li>2020 - <a href="./2020/">AI for Content Creation</a> (Workshop at CVPR 2020). Includes session video recordings!</li>
            <li>2019 - <a href="https://nvlabs.github.io/dl-for-content-creation/">Deep Learning for Content Creation</a> (Tutorial at CVPR 2019)</li>
        </ul>
        <br><br>
    </div>

    <footer class="bg-light text-dark py-5">
        <div class="container">
            <img src="images/logos/google_logo_tp.png" class="logo">
            <img src="images/logos/brown-cs-logo.png" class="logo">
            <img src="images/logos/NVIDIALogo_2D.png" class="logo">
            <img src="images/logos/Adobe-Logo.png" class="logo">
            <img src="images/logos/berkeley-logo.png" class="logo">
            <img src="images/logos/U-of-T-logo_blue.svg" class="logo">

            <p>
                Thank you to <a href="http://themestr.app">Themestr.app</a> and <a href="http://getbootstrap.com">Bootstrap</a> for the Webpage design tools.
            </p>
        </div>
    </footer>

    <!-- Javascript to control YouTube player-->
    <script>
        // From https://developers.google.com/youtube/iframe_api_reference
        document.addEventListener("DOMContentLoaded", function(event) {
            var tag = document.createElement('script');
            tag.src = "https://www.youtube.com/iframe_api";
            var firstScriptTag = document.getElementsByTagName('script')[0];
            firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
        });

        var morningplayer, afternoonplayer;
        function onYouTubeIframeAPIReady() {
            morningplayer = new YT.Player('morningplayer', {width: '800', height: '450', videoId: 'x6cHOulDXUo'});
            afternoonplayer = new YT.Player('afternoonplayer', {width: '800', height: '450', videoId: 'DJzMLa8zuJQ'});
        }
    </script>

  </body>
</html>
